# 超越文本：多模态检索增强生成的实现路径与业界实践综述

## 背景与动机

RAG 概念及痛点： RAG（检索增强生成）是将大语言模型与外部知识库相结合的技术架构 。传统大模型闭门作答如同“闭卷考试”，而引入 RAG 则相当于让模型开卷参考一个知识库 。具体而言，在模型生成回答前，先根据用户问题从海量文档中检索出相关信息，把这些资料作为上下文提供给模型，从而提高回答的准确性和依据性 。RAG 能有效缓解大模型的幻觉（凭空编造事实）和知识时滞问题，因为模型可以实时查阅最新的外部信息 。

单模态局限： 然而，早期的 RAG 系统几乎只处理文本模态的知识。现实中信息远不止以文字呈现：图片、图表、音频、视频等承载了大量关键内容。仅依赖纯文本会导致信息盲区——例如技术文档中的示意图、财报中的曲线图、医学影像等，如果 RAG 不支持这些模态，等于忽略了问题相关的宝贵信息。文本模型无法“看”图，自然无法回答涉及图像内容的细节。这限制了 RAG 在复杂场景下的适用性。

多模态 RAG 概念： 多模态 RAG正是为了解决上述问题而兴起的。它将 RAG 的理念扩展到图像、音频、视频等多种数据模态，为生成提供更全面的知识支撑 。简单说，多模态 RAG 允许输入可以是图像或文本，检索的知识库既包含文本也包含图片等，生成回答时模型能够综合多模态信息 。这使得 AI 系统可以从更广泛的数据源获取答案，突破单一文本模态的局限 。在我们的视觉时代，许多文件和场景本就是图文并茂的，多模态 RAG 能够捕获纯文本分析所遗漏的细微之处，利用视觉辅助来增强对复杂概念的理解，从而提供更准确、丰富的回答 。



为什么需要多模态： 引入多模态的必要性体现在多个方面：

- *信息完备性：*许多企业文档、研究报告包含图片、表格和图形。如果仅基于文本检索，图中的数据和内容将被忽视，导致回答不完整或不准确 。通过引入视觉模态，系统能理解图像中的关键信息，捕捉文本以外的上下文细节，例如图表趋势、物体外观等 。这样回答将更加全面。
- *准确性与专业性：*在专业问答中，图片常承载文字无法直接描述的信息（如医学影像的病变特征）。多模态 RAG 可以将医学影像与医学文献结合，让模型基于医学图像的细节检索相应专业知识，回答更加精准。研究表明，在需要外部知识才能回答的视觉问答任务（如 OK-VQA），结合检索的方式显著优于模型闭门作答 。
- *降低幻觉：*将图像等模态纳入 RAG，有助于进一步降低幻觉发生率。模型不仅有文字依据，还有视觉依据，回答更有理有据。另一方面，如果模型对图像理解不确定，可以从外部检索相关说明来辅助作答，减少胡猜的情况。
- *应用需求驱动：*用户提问日益多样化，可能上传一张图像提问细节，或者提及视频片段内容。为了让 AI 助手真正胜任这些场景，必须赋予其处理多模态信息的能力。例如，一位用户拍下博物馆展品图片询问背景故事，如果系统只能处理文本，将无法识别图片中的展品，更别提提供详尽答案了。

总之，多模态 RAG 的出现顺应了 AI 从“只能看文本”向“能看会听”的进化趋势。它让 AI 更加类似人类，可以通过视觉、听觉获取信息并据此回答。这为构建更强大、可靠的 AI 系统奠定了基础，使其能够利用动态的多模态知识库来提升性能 。



## 技术路径综述

多模态 RAG 要实现“能检索多模态数据+生成多模态答案”，目前主要有几种架构路径选择，每种路径各有原理和特点 。下面我们将分别介绍当前主流实现路径，并比较它们的优劣以及在业界的采用情况。主要的技术路径包括：

### 路径一：统一向量空间嵌入检索

原理：**将所有模态的数据嵌入到同一语义向量空间中，实现跨模态的相似度检索**  。

具体做法是训练或采用一个多模态嵌入模型，对文本和图像（乃至音频、视频帧等）进行编码，使它们的向量表示位于同一个向量空间中。这样，无论查询是文本还是图像，都可以在这个统一空间中与数据库条目的向量进行匹配，实现跨模态检索。

例如 OpenAI 提出的 CLIP 模型（Contrastive Language-Image Pretraining）就是典型，它通过对大量图文对进行对比学习，获得了能够将图像和文本投射到相近向量表示的模型 。使用 CLIP 编码后，就可以用文本查询直接检索相关图片，反之亦然。这条路线本质上复用了文本向量检索的方法，只是把原来的文本嵌入模型换成了能处理多模态的嵌入模型 。

特点和优劣：统一向量空间方案的主要优点在于**架构简洁统一**：现有的文本 RAG 基础设施几乎不需要大改动，只要用多模态嵌入模型替换原本的文本嵌入模型即可  。检索部分依然是向量相似度搜索，可以利用成熟的近似最近邻索引（如 FAISS、Annoy 等） 。生成部分，则将原先纯文本的大语言模型(LLM)替换为支持多模态输入输出的模型(MLLM)，从而在回答时理解检索到的图像内容  。整体流程和文本RAG类似，只是在嵌入和生成阶段做了“多模态化”的替换。这种一致性极大降低了实现难度——对于已有RAG系统的团队来说，升级到多模态RAG只需新增一个多模态embedding模型和采用一个多模态LLM。

然而，其挑战与缺点在于对**跨模态嵌入模型的要求很高**。模型必须有效地将不同类型的数据映射到同一空间，既要理解图像的视觉内容又要理解文本语义，并让相匹配的图文距离更近 。如果嵌入模型不到位，可能出现**检索误差**：例如用户查询一句话，向量空间可能把一张视觉上相似却语义无关的图片当成高相关项返回。尤其对于复杂场景，图像里可能含有文字、表格等细节，单一嵌入向量未必捕捉得到 。

换言之，这一路径把**难点前移到模型训练**：需要一个强大的多模态表示模型来“撑起场子”。目前这方面进展迅速，除了 CLIP，还有 Google 的 ALIGN、微软的 BiBERT 等跨模态模型，可用作嵌入模型 。工程上，开发者还需注意嵌入向量的维度和性能——多模态向量通常维数较高（如 CLIP 向量512维），索引大量数据时存储和检索效率都是考量因素。但这些有赖于底层向量数据库和硬件支持，技术上已较成熟。



实际案例：业界对这种方案兴趣很大，因为它提供了一条相对直接的多模态检索升级路径。一些产品和开源工具已经在用。例如：

- *Cohere Embed 模型：*知名 NLP 公司 Cohere 发布了多模态嵌入模型 Embed-3/4，支持将文本和图像转换为同一向量空间中的向量  。他们宣称该模型可用于语义搜索、RAG 等场景，开发者只需调用 Cohere API 获取向量，即可对图文混合的数据集做统一检索 。这在企业应用中非常方便——比如电商公司用它将商品图片和描述一起向量化存入向量库，用户无论提供一张类似商品的图片搜索还是输入文本关键词，都能检索到对应商品的信息 。
- *Databricks 向量搜索：*Databricks 平台在 2025 年展示了如何利用多模态嵌入实现医疗文档的搜索 。他们将包含文本和医学影像的病人病例数据一并转成多模态向量存储在 Delta 表中。这样，通过一个文本查询就能搜索到相关的影像报告，**使包含图像和文本的复杂 PDF 实现端到端搜索** 。这证明了统一向量空间在行业场景的可行性：多模态嵌入为医疗等需要“全卷扫描”多种信息的需求提供了新方案。
- *LlamaIndex 多模态索引：*开源框架 LlamaIndex 也支持创建 MultiModalVectorIndex，将文本段落和图像一起索引到底层向量库中  。它默认集成了 CLIP 作为嵌入模型 。开发者用统一接口即可检索文本和图片两类内容。这降低了多模态应用的开发门槛，使个人开发者也能尝试构建自己的跨模态 QA 系统。

**业界应用情况：统一向量空间路径在检索阶段体验出色**，因此在需要**快速跨模态搜索**的应用中较受欢迎（如企业知识库搜索、素材管理等）。由于**实现改动小**且有现成模型可用，许多新项目倾向于先采用这一方案进行多模态功能原型开发 。特别是像 Cohere 这类第三方服务提供易用的多模态向量服务，让更多团队愿意尝试。

不过，需要强调的是最终生成答案阶段仍需要一个多模态模型去理解图像内容，否则检索回来的图像对纯文本模型来说只是个占位符而已。因此，在实际落地中，**统一嵌入检索常常与多模态生成模型**配合使用：检索取出相关文本*和*图像，然后交给例如 GPT-4V 这样的模型阅读文本、查看图像，综合生成答案  。这一点在后文案例部分还将详细讨论。



### 路径二：模态归一（跨模态转文本）

**原理：将所有非文本模态的数据转化或对齐到文本模态**，使后续检索与生成过程都在单一的文本空间内进行  。这通常意味着对图像、音频、视频等在预处理阶段提取出描述性的文本信息，然后将这些文本与原有文本资料一起建立索引。如此一来，RAG 管道可以仍然**以文本检索为核心**。简单来说，就是把图像当做一段文字来处理。

具体实现上，对于图像，常见做法是：

- **OCR（光学字符识别）：**如果图像中有文字（如文档扫描件、截图中的字幕），先用 OCR 提取文字内容，以文本形式保存。这解决图像中包含的显性文字信息。
- **Caption（图像描述）：对于一般图片或图表，没有直接的文字可提取，则使用图像描述模型生成文字描述**（Caption）或摘要。例如用预训练的视觉语言模型（BLIP、LLaVA 等）让模型看图输出一段文字，说明图像中是什么、有何重要信息  。对于数据可视化的图表，可能需要特殊模型如 Google DePlot或微软 Pix2Struct将图形转成表格数据或文字概述  。
- **元数据补充：在描述文字之外，可以为图像生成一些结构化的标签或属性**。例如给照片打上地点、人物、物体标签；给图表记录坐标轴含义、单位等。作为文本存入数据库的元数据，辅助检索匹配 。

完成以上步骤后，系统就能把每张图像“替换”成若干段文本描述。这些文本和原有文本资料一起进入**索引库**（通常还是向量数据库，但此时只有文本向量）。当用户提问时，也按普通文本 RAG 流程处理：将问题向量化，在文本+图像描述混合库中检索相关的文本片段，然后把它们交给生成模型回答 。只是如果检索结果中某段文本其实来自图像描述，生成模型回答时可能需要注意来源不同，比如有时需要引用图像说明。



优点：这种“模态归一”的方案实现难度低、利用了成熟的文本技术栈。其优点可以概括为：

- **充分复用文本 RAG 优势：**所有检索都基于文本语义匹配，能直接利用强大的文本嵌入模型（Sentence-BERT 等）和快速文本搜索技术。这些技术非常成熟，效果和效率都有保障  。相比训练新的多模态嵌入模型，文本向量模型获取容易且可控。
- **简化多模态融合问题：**不同模态信息都变成了文字，自然就在同一语义空间，无需额外解决跨模态对齐。检索阶段不用担心图像和文本得分如何归一，因为一切都是文本在比较相关度。**重排序**也简单很多，不需要特殊多模态排序算法。
- **降低工程复杂度：**开发团队不需要引入复杂的多模态模型，只需做好预处理脚本和OCR/标注工具调度。检索、数据库、后端服务这些基本保持不变 。对于资源有限或想快速上线的项目，这是性价比最高的路径。
- **回答的客观准确性：因为图像转成了客观的文字描述**，在回答客观问题时非常有用 。例如一张统计图表转成了数据表文字，那么回答数值型问题时模型只要从文字找答案，不会因为自己看不懂图而乱猜。 中的经验表明，对于客观题（如图表里的数值、明确事实），这种预生成的文字信息非常可靠。



缺点：当然，该方案也存在明显的局限和权衡：

- **前处理成本高：**将大量图像转文字是一项昂贵的工作。OCR 可能出错或需要人工校对，图像描述模型生成的文字质量也参差不齐。并且预处理所有图片需要时间和算力。如果知识库中的视觉数据经常更新或规模庞大，持续维护这些文本描述是一笔不小的成本 。
- **细节和直观损失：文字描述往往无法涵盖图像中的全部细节**或直观感受。模型生成的描述可能遗漏背景细节、色彩等，而这些在某些问答中可能正是关键。例如描述一张名画，文字也许能写出画的主题，但画风、色调等信息难以完全表达。一旦舍弃了原始图像，回答就只能基于描述内容，**潜在信息损失**无法挽回 。
- **对生成模型要求高：虽然检索是文字，但生成阶段如果还是用纯文本模型，它并不真的“见过”原始图像，只看描述可能不够**。如果问题需要非常精细的视觉判断（如“图中患者X光的阴影形状是否异常”），除非描述写得极其详细，否则文本模型难以给出准确判断。因此很多情况下仍需一个多模态模型最终查看原图。但这又部分抵消了前面简化架构的优势（详见下一节的混合方案）。



工程技巧：为充分发挥路径二的优势，工程上有一些实用策略：

- *预处理分而治之：*可根据图像类型选择不同处理方法，提高描述质量  。例如在文档图像中，OCR 必不可少；对于包含复杂图表的图片，可先分类出“图表类”图像，用专用模型（如前述 DePlot）把图形量化为表格文字，再对其他一般照片类图像用常规图像caption模型生成描述  。这样每种图片都用最合适的方法处理，生成高价值的文本信息。NVIDIA 的实践就采用了一个模型集成思路：用一组模型分别处理不同类别的图像，再汇总结果  。
- *保存图像索引以备后用：*虽然主要检索基于文字，但最好仍存储原始图像或其引用（如文件路径、URL）并与文字描述关联。这样在生成答案时，如果需要引用或展示图像，还能取出原图。即使大语言模型本身不会输出图像，我们也可以在答案里提供图像链接或由前端显示原图以佐证答案。举例来说，一个企业文档助手回答问题时，不仅给出文字解释，还可以把相关图表原图附在答案里供用户查看，以增强可信度。
- *定期更新和审校描述：*知识库不是一成不变的，图像描述也需要随数据演进更新。建立自动化管道监听数据变更，重新生成嵌入和描述。有条件的话让人工参与审阅关键图像描述，尤其在医疗、法律等高风险领域，确保描述准确客观。
- *结合简单视觉信号：*对于某些问答，不需要整图解析，只要简单视觉特征即可。例如产品搜索里，只需颜色或形状信息。这种情况下，不妨在文字索引里加入图像的简单标签（如颜色=红，类别=“鞋子”等），通过这些标签过滤候选，从而减少纯文本匹配的压力。这也是一种广义的“模态转文本”。



实际案例：这一路径在业界非常常见，尤其在处理文档类、多媒体资料的问答场景中：

- *文档问答助手：*许多面向 PDF、报告的问答产品都采用此法。例如 OpenAI 的插件工具和 LangChain 的 Unstructured 库可以解析 PDF 文档，抽取出文本、表格甚至图像，将图像另存为文件并进行 OCR 文本提取  。这样整个 PDF 就变成一组文字块，后续问答时只需在这些文字块中检索答案。微软 Azure 的认知搜索服务也有类似的“Skill”管道，对文档里的图像自动执行 OCR，把结果并入可检索文本。 提到，典型临床文档往往包含扫描图像，需要先 OCR 才能检索其中信息，这正是业界普遍做法。
- *数据报表分析*: 企业经营报告中的图形、表格通常通过截图或嵌入图像形式存在。一些 BI 问答系统会在索引阶段线下将图表转成数据表文本，如“年份=2021, 销量=1.2亿”，并在检索结果中把这些文字提供给 LLM。这样当用户问“去年销售额是多少”时，模型可以直接从文字找到答案，而不用自己去“看”图估读 。这一方法在财务分析、市场报告问答中非常实用，提升了准确性。
- *百科问答*: 维基百科等知识库的某些条目含有丰富的图片、地图。这类问答机器人如果实现多模态，通常也倾向于预先为图片生成描述嵌入到条目文本中。当用户问到与图片相关内容（例如“图中人物穿什么服饰”），模型其实是在检索到条目中文描述里写的服饰细节来回答的，而非临时让模型看图判断。这样做确保了一致性和可重复性。事实上，维基百科很多图片都有文字说明（alt 文本或标题），这些在多模态 RAG 中可直接当作描述使用。

业界采用情况：由于实现简单又可靠，**路径二是当前业界落地最多的方案之一** 。特别是在问答对实时交互效率要求高、对绝对准确率要求没那么极端的应用中，模态归一的方案提供了一个折中且易用的选择。许多公司现有的知识库 QA 产品，本身就没有引入图像分析能力，但通过预处理图像为文本，快速增加了“看图”的表面功能，满足业务需求。需要强调的是，这种方案在回答客观问题时效果很好，但对于涉及主观判断或复杂视觉推理的问题（比如“这张照片里的人开心吗？”），单靠文字描述往往不足。这也是为什么当应用场景需要深度视觉理解时，业界会引入下一种更强大的混合方案。



### 路径三：分离检索与多模态重排序

原理：第三种主流路径采用**“分库检索，融合排序”**的策略 。它的核心思想是：**不同模态各用最适合自己的方式进行初步检索**，然后再通过一个多模态判断模块将这些结果进行相关性融合，选出最终的答案支撑材料。这有点类似搜索引擎里的 rank-rerank（先分别排序，再整体重排） 。



如何实现呢？典型流程如下：

1. **独立索引和检索：**为每种数据模态建立单独的索引和检索通道。例如，准备两个向量数据库：一个存文本资料，使用文本嵌入模型；另一个存图像资料，使用图像嵌入模型（可以是统一空间的模型，也可以不同模型，只要能各自衡量相似度）。对于音频、视频也可类似分开索引。用户提问进来后，并行地对每个索引执行检索，各自取出相关的 Top N 条目。
2. **多模态重排序（融合）：将上述各路检索得到的候选结果汇合在一起（假设有 M 个模态，每路 N 条，则候选总数 M×N）。接下来用一个多模态相关性模型**对这些候选进行评估排序，选出最相关的 K 条作为最终检索结果 。这个多模态模型本身能够同时考虑文本和图像等信息，可能是一个训练好的小型多模态 Transformer，或者直接利用像 GPT-4V 这样的强大模型来判断。例如，它可以读取一个文本段和一张图，判断两者与查询的匹配度高低，然后据此给候选排序。通过这样的重排序，理想情况下，来自不同模态的结果可以公平比较，优中选优。最后这些融合后的 Top K 内容再送入生成模型构成答案。
3. **生成回答：**最后一步与之前类似，将融合结果交给 LLM/MLLM，基于它们生成最终回答。如果生成模型是多模态的（如 GPT-4V），可以直接把文本和图像一起给它；如果是纯文本模型，也可以选择将图像用描述文字替换后再输入（但这样就退化回路径二了，不是最佳选择）。



优点：分离检索&重排序法结合了前两种路径的长处：

- **专模态专用最佳技术：**各模态的检索可以用各自领域效果最好的方法，而不必强行统一或转化。例如文本用最强的文本语义搜索，图像用最强的图像embedding或图像标注搜索（甚至可以用传统以图搜图技术）；音频可以用声纹搜索等。大家各展所长，**减少损失和偏颇**。这避免了一刀切模型可能带来的某些模态性能下降问题 。
- **减少信息损失：**与路径二不同，这里不会把图像扔掉或转文字——图像始终以图像形式参与到最终排序和回答。视觉信息得以保留到最后，相当于给生成模型看了“原料”，因此回答有潜力更加精准和详细。
- **无需多模态嵌入模型：这一方案中，并不需要一个通吃图文的embedding模型，各索引可以用各自单模态模型**，因此避免了训练或获取多模态嵌入模型的麻烦 。对于手头已经有不同模态数据及各自搜索方案的团队来说，这种架构易于组合利用现有组件。
- **扩展到更多模态容易：**理论上可以拓展到任意多种模态，各自检索后一起融合即可。新加一种模态不影响原有检索逻辑，只是在融合时多处理一些候选，**模块化扩展性**好。



缺点：然而此方案也同时引入了更高的系统复杂度和**计算成本**，主要体现为：

- **实现复杂：**相比前两种，这是对系统架构和算法要求最高的。要维护多套索引系统，还要开发一个多模态融合模型。融合模型可能需要针对业务场景训练，涉及标注不同模态结果的相关性、调优排序算法等。整个流水线的步骤和组件都变多，更难调试和维护 。
- **检索效率较低**：每次查询都要跑多路检索，然后排序。如果有很多模态，每路N=10，那总共就有50、100个候选需要重排，耗时和算力代价都不容小觑。特别是融合模型本身如果很大（如用 GPT-4V 直接打分候选），那每个候选都得推理一次成本非常高。可以预见查询延迟会比前两种方案明显增加。
- **融合判别困难**：如何让一个模型来比较“文本段A vs 查询”和“图像B vs 查询”，哪个更相关？这是个开放挑战。不同模态间缺少统一的相关性度量标准。例如查询“这药片是什么”，一个检索到的文本候选可能直接写着药名，图像候选是一张药片照片——哪一个更“相关”？理想上都重要，但排序模型需要决定先后。训练这样的模型需要跨模态相关性数据，且评估起来也比较复杂（涉及主观判断）。如果融合模型不够好，可能导致整体效果甚至不如简单的单一路径。
- **资源占用：**除了实时性差，路径三对存储资源要求也更高，需要维护多份向量数据库（文本、图像…），而且因为不统一空间，无法合并索引，只能多份独立存。对于数据量大的应用，这意味着更多的存储和运维开销。



工程要点：在实践这种架构时，有一些工程策略可以帮助缓解问题：

- *候选数控制：*不一定每种模态都取N个候选，可以根据查询或模态重要性灵活调整。如果对于某类问题，图像通常作用小，可以只取极少的图像候选，从而减少重排负担。或者可以先用查询意图分类模型判断此次提问需不需要看图/看音频，再决定是否检索某些模态索引。
- *轻量融合模型：*尽可能训练或采用轻量级的融合模型，例如一个多模态 BERT 或 small Transformer，对每个候选输出相关分数，而不是调用庞大的GPT模型做判断。也可以考虑先用简单线性融合（给不同模态结果预设权重）做一个初步筛选，再用复杂模型精排较小集合，以减少计算。
- *结果缓存和合并：*对于高并发系统，可以考虑对固定的问题或热门查询缓存融合排序的结果，避免每次都全流程跑。此外，如果文本和图像经常同时出现且关联紧密（如百科条目文字配图），在索引时就将它们绑定为一个复合文档，检索阶段作为整体返回，这样融合时自然把互补的信息放在一起，有助于模型判断。



实际案例：由于此路径复杂，一般在学术研究或大型产品中能看到部分思路的应用：

- *学术方案 (知识型VQA)：*在需要外部知识的视觉问答（Knowledge-based VQA）任务中，有论文探索了**先检索文本资料+检索图像资料，再融合回答**的方法。例如 OK-VQA 挑战赛上有团队将查询问题送入一个知识库（如维基百科）检索相关文本，同时也检索一个图像库获取可能有用的图片，然后用一个多模态模型读这些文本和图片共同推理答案。这本质上就是路径三的思路。这些研究表明，相比仅文本检索，结合图像检索在某些视觉常识题上确实能提高准确率，但关键在于如何有效融合 。一些工作甚至尝试让生成模型自己在回答时决定引用哪个文档、哪个图片（通过引入强化学习或权重调整）。
- *搜索引擎问答：*大型搜索引擎（如 Google、Bing）在回答用户复杂查询时，其实也有融合多模态结果的意味。比如当用户问“这幅画现藏于哪个博物馆？”并上传一张画作照片，后端可能一方面用以图搜图找相似图像识别画作名称，另一方面在文本索引里查询画名找博物馆信息，然后把两个结果拼合形成答案。这种幕后流程可以被视为一种简单的双模态融合。再比如 Bing Chat 显示文本回答的同时常附上一些相关图像，那是搜索引擎在文字答案外挑选的视觉结果，一并提供给用户参考。
- *多模态数字助手：*一些前沿应用（如微软 Office Copilot 等）需要在复杂文档中回答问题。Imagine一个 PPT 问答助手，用户问“销售图表反映了什么趋势？”。系统可能做的是：文本索引找到那页幻灯片的讲解文字，图像索引拿到图表图片，然后由一个多模态模型结合图和文字总结趋势。这与路径三相符，已在特定插件中初步实现。微软早期的 LayoutLM 等文档理解模型也体现了把图像和文本特征融合来理解文档的思想。

**业界采用情况：目前纯粹的路径三在工业界尚不是主流**，原因如上所述是实现成本较高。很多团队会尝试折中方案，例如“路径二 + 在生成时附带图像”：即用文本检索为主，但最后让生成模型看看相关图片辅助回答（这其实类似路径三的简化版：候选融合时只在最后一步）。这种折中能享受部分多模态融合好处，又避免了独立重排序模块。不过，随着多模态模型能力提升和需求增长，我们可能会看到更多厂商投入此类架构的研发。如果用户问题确实需要非常准确地综合多源信息，那么不惜代价的路径三也有其用武之地。在未来趋势部分我们也会讨论，如何降低多模态融合的难度可能是一个重要研究方向。



### 路径比较与选择

综合来看，这三种路径各具特点，在性能、成本和工程复杂度上有所差异，适合的场景也不同：

- **性能（效果）:** 如果不考虑代价，路径三潜在效果最佳，因为它保留了各模态信息并精细融合，理论上能找到最相关的综合结果，回答也最准确详实。但它的优势发挥依赖于融合模型非常强大，否则收益有限。路径二在客观问答上效果好，但遇到需要深度视觉理解的问题会力不从心。路径一性能介于两者之间：在多模态embedding模型足够好的前提下，它能够较准确地检索并利用图像信息，但如果embedding有偏差，效果会下降。总体而言，**简单问题三者区别不大**，复杂跨模态推理问题则需要三 > 一 > 二的能力梯度。
- **成本（推理开销）:** 路径二成本最低，因为除了预处理外，查询时和文本RAG几乎一样快速，没有额外大模型调用。路径一增加了一次多模态embedding计算（对查询或者对库内容预先算好），开销稍高但通常可以接受。路径三最昂贵，要多路检索+多次模型推理融合，特别如果融合用大模型，那费用成倍上涨。因此在注重**实时性或低成本**应用里，路径二、一更有优势。
- **工程复杂度:** 路径二最简单，实现步骤与传统RAG差别不大，多了预处理流水线。路径一也不复杂，只需替换模型，但是要获得好的embedding模型可能需要训练或借助外部API，这部分工作量中等。路径三对团队要求最高，需要全链路设计新的算法和模块，涉及多模态机器学习的综合知识，所以只有在确有必要时才会考虑实施。



业界常用路径：结合以上，当前业界优先采用的是**路径二（文本归一）或路径一（统一嵌入）**，因为它们性价比更高  。尤其是在产品验证阶段或需求不极端的情况下，先用路径二快速达到“看似能处理图像”的效果是常见做法。然而，随着像 GPT-4V 这类强多模态模型的出现，越来越多团队开始尝试路径一，即直接使用多模态embedding和模型，让系统端到端地“看”图片。这在一些需要处理海量多媒体数据的公司中越来越流行，例如社交媒体内容搜索、数字资产管理等，因为统一embedding方便构建大规模索引和搜索 。至于路径三，目前多见于研究原型或特殊用途产品（如高精度医学咨询机器人）中，行业落地案例相对较少。不过，也有观点认为路径三可能是未来方向，一旦技术足够成熟，它将在对准确率要求极高的场景中发挥作用。

接下来，我们将通过分析一些实际落地案例，看看各大公司和项目在多模态 RAG 上的实践，以及他们倾向于哪种技术路径。



## 实际落地案例

本节针对核心问题4，列举多模态 RAG 的业界实践，涵盖 OpenAI、Meta、Google、Cohere 等知名公司和部分国内大厂，以及一些开源项目。我们也关注在不同场景（如医疗、企业文档、视频检索）的应用，了解成功经验和教训。



### OpenAI GPT-4V：视觉与 RAG 融合

GPT-4V 简介：OpenAI 的 GPT-4V（即 GPT-4 Vision）是 2023 年发布的 GPT-4 多模态版本，能够接受图像和文本输入，输出文本响应 。作为迄今最强大的多模态大模型之一，GPT-4V 可以看图回答问题、描述图像、理解含有文字的图片等 。GPT-4V 本身属于生成模型部分的突破：它在 GPT-4 架构中融入了视觉处理模块，使之成为同时具备视觉和语言理解的模型 。这使得 GPT-4V 在多模态任务上表现出色。然而需要指出：GPT-4V 并非内置检索系统，它不会主动从互联网或数据库检索信息，而是依赖用户提供的图像和文字作为输入。在 ChatGPT 场景下，GPT-4V 看到的仅是用户上传的图片和聊天上下文。如果需要外部知识，仍然要靠开发者将检索到的内容馈送给模型。

GPT-4V 与 RAG：尽管 GPT-4V 本身不执行检索，其出现仍极大地推动了多模态 RAG 的发展 。开发者迅速探索将 GPT-4V 集成进 RAG 管道的新方案，典型做法是：使用 GPT-4V 作为生成端模型，与一个外部检索组件结合 。具体流程类似路径一/三的融合：

1. 用户提出复杂问题（可能有图片或需要图片作答）。
2. 系统先对文本查询部分用文本向量检索知识库获得相关文本段落。如果问题中包含图像或需要图像信息，则可能还会用图像embedding检索相关图像（或者通过关键词检索图片库）。
3. 最后将所得的文本内容和图像都提供给 GPT-4V，让它整合同时考虑这些信息，生成最终答案  。



GPT-4V 在此充当了一个**强大的多模态信息整合者**：它可以阅读检索到的文本段落，也能“看到”相关图像，然后基于二者产生上下文丰富的回答 。这使回答不仅有文字依据，还能引用图像佐证。例如回答中可以描述“如附图所示…”，用户甚至可以真正看到附图。

架构细节：由于 GPT-4V 是闭源模型，我们无从得知其具体架构细节。但根据 OpenAI 论文和开源类似模型的推断，GPT-4V 很可能在 GPT-4 的 Transformer 主干中加入了视觉编码器。

一种常见架构是使用一个预训练的视觉 Transformer（如 ViT）将图像编码为一系列视觉特征向量，然后通过投射层将这些向量嵌入到 GPT-4 的输入序列中，使模型把它们当做特殊的“图像token”处理。这与一些开源多模态模型如 LLaVA 的做法类似：LLaVA 就是用 CLIP 的视觉编码器处理图像，并在其 LLaMA 模型的Embedding层加了线性投影，将图像特征融合进去 。GPT-4V 可能采用了更复杂的预训练策略和结构，但思路类似。通过这种架构，GPT-4V 获得了卓越的视觉理解能力 ：它不仅能识别图片中的物体，还能读懂图片里的文字、分析图表内容，甚至对手绘草图进行推理。这些能力使 GPT-4V 在多模态 RAG 中发挥关键作用。举例来说，在一个技术文档 QA 系统里，GPT-4V 可以被用来解析文档中的截图（读出其中的报表数字或界面内容），然后结合文档文本来回答用户问题——这一点是纯文本模型无法做到的。

应用实例：GPT-4V 自推出后已经在多个场景中展示了其实力：

- *ChatGPT 场景：*很多用户使用 GPT-4V 来分析身边的图片，例如让它阅读一张收据图片并总结消费，或者看一副数据图表给出结论。这其实就是小型的多模态 RAG：知识来源是图像，GPT-4V 将其转成了答案 。一些结合了插件的 ChatGPT 场景，则更进一步让 GPT-4V 利用检索：先用插件查资料，再让 GPT-4V 综合查到的资料和用户图片回答。这证明 GPT-4V 可以和检索组件协同工作。
- *Be My Eyes（视障助手）：*OpenAI 与 Be My Eyes 合作，用 GPT-4V 为视障用户提供图像识别和问答服务。用户可以拍摄身边物品或环境照片，GPT-4V 不仅描述图像，还回答后续问题。如果涉及知识性的问题，例如用户拍了一本书的封面问“这本书作者还写过哪些书？”，系统可能会先OCR出书名，然后检索作者作品列表，再由 GPT-4V 读回这些文字告诉用户。这背后体现了一个多模态 RAG 的闭环：图像提供问题背景，文字检索带来外部知识，GPT-4V 贯通两者形成回答。
- *GPT-4V + 企业数据:* 一些企业开始尝试用 GPT-4V 来构建内部多模态问答系统。例如将产品手册（包含图片说明）交给 GPT-4V：用户提问时，系统检索相关手册页文本和插图，由 GPT-4V 给出带图的解答。在这个过程中GPT-4V显著提升了回答对图例的引用和解释能力  （有报告称 GPT-4 32k 模型相比 GPT-3.5 能更好地引用图像信息  ），从而让答案更直观。虽然这些应用细节未大规模公布，但可以肯定许多团队已在实验这种强大的配置。

业界影响：GPT-4V 的出现可谓多模态 RAG 的里程碑。它证明了大模型可以成功融合视觉语言，从而让 RAG 真正跨入图文并重的时代 。受其启发，业界和开源界纷纷跟进开发多模态大模型（后面将介绍）。对于构建 RAG 系统的人来说，GPT-4V 提供了一种新的强力工具：过去我们也许会用 GPT-4 + OCR 结果作答，现在我们可以直接用 GPT-4V 看原图作答，逻辑更简单，回答质量也往往更高。这并不意味着传统 OCR 或embedding没用了，而是我们在系统设计上有了更多选择。例如可以权衡：是提前用 OCR 转文本减少 GPT-4V 工作量，还是干脆让 GPT-4V 自己在对话中OCR图片？这种灵活性是前所未有的。总而言之，OpenAI 的 GPT-4V 让多模态 RAG 真正落地成为可能，并树立了一个业界标杆。



### Meta 与开源社区的探索

Meta（原 Facebook）动向：作为AI研究的巨头，Meta在多模态RAG相关领域也有许多布局。虽然 Meta 没有像 OpenAI 那样直接推出一个对外的GPT-4V模型，但他们在多模态表示和模型方面的研究为多模态RAG打下了基础。

- *跨模态表示:* Meta AI 在 2023 年发布了 ImageBind，这是一个可将图像、文本、音频甚至传感器数据绑定到统一嵌入空间的模型。ImageBind 能够让不同模态的输入直接在同一向量空间里比较相似度，被视为 CLIP 的泛化。这对于多模态检索非常有用：使用 ImageBind，搜索引擎可以实现“声音找相关图像”“图像找文字”等功能。比如输入一段鸟鸣音频，它可以检索到对应鸟类的图片或名称。ImageBind 这种统一嵌入正契合了路径一的理念，将各种模态的信息融合表达 。

  可以预见 Meta 若构建自家多模态RAG系统，ImageBind 这样的技术会是关键组件，用于索引Facebook海量的多媒体内容（图片视频和文本）。

- *多模态模型:* 在生成模型方面，Meta 虽然目前没有官方发布类似 GPT-4V 的模型，但其开源的 LLaMA 系列给社区提供了基础。有研究者在 LLaMA 上添加视觉模块，训练出 LLaVA（Large Language and Vision Assistant）等模型 。LLaVA 结合了 ViT 图像编码和 LLaMA 的语言能力，能够执行图像问答任务 。这一模型并非 Meta 官方，但使用了 Meta 的基础模型，让社区版本的“GPT-4V”成为可能。

  此外，Meta 自己的多模态研究还包括 FLAVA（Facebook多模态模型）等，它们探索图文统一建模，对多模态RAG有理论指导意义。还有消息称 Meta 正在研发 LLaMA 的多模态后续版本（俗称 *“LLaMA-3”* 或增强版)，很可能会加入视觉能力，届时 Meta 也会有与 GPT-4V抗衡的模型。在检索增强方面，Meta AI 团队也关心如何让 LLM 利用外部知识。例如 2022 年提出的 Retro、Atlas 等都是检索增强模型，只不过当时聚焦文本。

  可以想见，一旦 Meta 拥有多模态基础模型，他们会尝试将检索增强拓展到图像等数据上，毕竟 Facebook/Instagram上每天产生如此多的多媒体内容，对 Meta 来说多模态 RAG 有直接应用价值（如内容审核、多媒体搜索等）。



开源项目：开源社区对于多模态RAG相当活跃，既有模型也有方案涌现：

- *LLaVA 和衍生模型:* LLaVA 是开源社区基于 LLaMA-7B 教育而成的视觉问答模型，能力类似 GPT-4V 的入门版。它常被用于多模态应用的试验，例如在文档问答项目中，用 LLaVA 来生成图像的文字摘要或直接参与问答  。微调版的 LLaVA 还能执行简单的多模态 RAG：比如给它提供一些图像描述文本和图像，它可以整合回答。除了 LLaVA，还有如 Mini-GPT4、BLIP-2 等一些组合式模型，它们用预训练视觉encoder接上中型语言模型，形成一个多模态对话模型。这些模型虽然能力不及 GPT-4V，但胜在开源可控，可以部署在本地或私有环境中，在企业内部做多模态检索问答时有用武之地。
- *Pix2Struct（Google 开源）:* 这是 Google 提出的图像转结构化文本模型，特别擅长从图表中提取信息 。Pix2Struct 可以将输入的图像（如柱状图、UI 截图）转换成一系列文本串，表示图像的结构和内容。NVIDIA 的案例就使用了 Pix2Struct 来处理 PDF 中的复杂图形 。开源的 Pix2Struct 模型可以拿来与 RAG 管道结合：预处理阶段用它把图表转成可索引文本，这比一般OCR或Caption专业得多，因为它能识别坐标轴、数据点等。对于涉及大量图表的知识库（金融、科研文献等），Pix2Struct 是个绝佳工具，提高了路径二的质量。
- *Kosmos 系列:* 微软研究院发布了 Kosmos-1（2023）和后续的 Kosmos-2，是尝试让 Transformer 模型同时具备视觉-语言-声音处理能力的探索。Kosmos-1 已能执行简单图文任务，而 Kosmos-2 reportedly 增强了 OCR和视觉推理 。这些模型显示了统一模型直接处理多模态的前景。虽然目前开源的 Kosmos 版本没有公开可用，但它们的思想推动了多模态RAG的统一模型路线。比如一个 Kosmos 模型本身就能看图听音并回答，那么未来 RAG 可能只需单一 Kosmos 模型连着数据库就能实现多模态问答，无需拼接多个组件。
- *RAG-Anything:* 这是一个由社区开发者（HKUDS 等）开源的“全家桶”式多模态RAG项目  。RAG-Anything 集成了文档解析、OCR、向量库、OpenAI 接口等模块，提供一套高层API帮助用户把各种格式的文件（PDF、图像、Office文档等）转成知识库并进行问答。它可以选用不同解析器（MinerU、Docling）来处理输入文件，将图像OCR、表格提取都做好，然后调用 OpenAI 模型生成回答  。这个项目的意义在于工程集成：它证明通过组装已有最佳工具（OCR、解析、LLM），任何人都可以搭建自己的多模态RAG应用，而不需要深厚的AI模型训练功底。RAG-Anything 在 GitHub 上提供了丰富的示例，涵盖从简单图片问答到Office文档QA等，展现了多模态RAG的广泛用途。



国内进展：国内大厂和研究单位也在积极投入多模态RAG相关研发：

- *阿里巴巴通义 Qwen-VL:* 阿里于2023年开源了 Qwen-7B 大模型及其多模态版本 Qwen-VL  。Qwen-VL 能接受图像+文本输入，在开源测试中表现不俗，可用于图像描述、视觉问答等。它还支持将多个图像和多轮对话结合，适合复杂场景。阿里或将此用于电商领域：用户给商品照片，模型识别商品并从数据库检索相关信息，回答用户提问。这类似多模态商品客服，一张商品图配合商品知识库文字，Qwen-VL 可以担纲生成回答。
- *百度文心多模态:* 百度的文心大模型（ERNIE）据报道也有多模态升级方向。文心在2023年曾展示图像理解能力，如给定一张画作图片，文心可以描述并结合百度百科回答相关知识问答。这背后想必用了百度自身的图像搜索/识别技术和文心模型的结合，相当于路径三的一个实例。另外，作为搜索巨头，百度在搜索中早已支持图文混合：比如在手机百度上搜菜品图片，可以得到菜谱说明，这是视觉检索+文本回答的服务。可以预期百度会把这种能力融入其文心一言对话中，让用户直接在对话中享受多模态搜索增强的回答。
- *华为、腾讯等:* 华为的盘古大模型也有计算机视觉分支，未来是否结合NLP尚未可知。腾讯优图实验室等有强项在OCR和图像识别方面，也许会与腾讯的ChatGPT类模型混合产生应用。例如在微信领域，可能出现能看图聊天的多模态助手，用于识别小程序码、截图内容等并提供说明（这其实也是一种RAG：小程序码图像解析为结果，然后回答用户）。国内还有一些创业公司在做垂直领域多模态问答，如医疗影像AI问诊助手、安防视频解析助手等等，都可以看作多模态RAG的具体场景应用。





场景实践：以下列举几个典型场景中多模态RAG的应用价值：

- **医疗:** 医疗领域数据多为图文结合：医学影像（X光、核磁）+ 医生报告、电子病历文本等 。多模态RAG在此可用来构建临床决策支持系统。比如医生上传一张患者的胸片并询问可能的诊断，系统先用计算机视觉模型分析胸片病灶，然后检索医学文献/指南中相似案例或病征描述，将图像发现和文本知识相结合给出诊断建议。这类似**知识增强的医学影像问答**。Databricks 的医疗示例也提到，将一整份包含文字和扫描件的病历PDF变得可搜索 。临床检索中，有时医生需要找既往病例的影像进行比对，如果实现文字+影像embedding共同搜索，就能输入“肺部肿瘤”检索出过去类似的CT影像与报告供参考。总的来说，在医疗这种专业高风险场景，多模态RAG有潜力提升信息获取效率，但也对准确性要求极高，需要谨慎验证。
- **企业文档:** 企业内大量知识存在于文档，且包括 PPT、PDF 等，其中插图、图表丰富。员工提问往往涉及报告里的图形内容。多模态RAG可以使这些**文档中的非文本信息被索引和引用**。例如，某产品运营报告里有市场份额饼图，当领导问“我们在某地区市场份额是多少”时，如果系统只看文本也许找不到答案，但通过图像解析找到饼图上该地区对应的百分比，就能准确回答 。这在商业智能问答中是很实用的功能。再如专利文档有大量示意图，研发人员问技术细节时，系统可检索相应图示并结合说明文字回答，还可以把图示一起给出。如此一来，复杂技术问题的解答将更直观易懂。可以预见，未来办公软件会集成多模态RAG：员工打开聊天助手，问“请解读一下这份PPT的图表”，助手通过读取PPT XML内容和图形生成文字说明，让员工快速掌握PPT要点。
- **视频检索与问答:** 视频作为时序多模态数据，包含视觉画面和音频（对白）。在视频内容问答或检索场景，通常做法是**结合视频字幕和关键帧**：先将视频音频转写成文本（字幕），用文本RAG获取初步结果；同时分析视频关键帧或镜头缩略图，用图像embedding找与问题相关的帧画面。然后将匹配的字幕片段和帧画面一同提供给多模态模型生成回答。例如在一个会议录像问答系统中，用户问“演讲者在PPT第3页提到了什么数据？”，系统可以通过字幕找到“这里我们达成了销售增长20%”之类的句子，并找到当时 PPT 画面的截图，然后让GPT-4V看着截图（上面有具体数字表格）和字幕，一起回答：“演讲者提到根据PPT，去年销售额增长了20%。”。这样的系统原型已经在一些研究中出现，将来有望在在线教育视频、安防监控分析等领域应用。安防中甚至可以问“昨天进入机房的人都有谁”，系统从监控视频帧里检索人脸并结合门禁记录文本给出答案，这背后也是多模态RAG（图像人脸+身份文字）的思路。可以说视频问答是多模态RAG的综合考验，需要图像、语音、文本齐上，但一旦实现，将为多媒体内容管理带来革命。

以上案例展示了多模态RAG的广阔应用前景。各大公司和开源项目都在探索最优实践。有的偏向技术创新（如 GPT-4V 提升模型能力），有的偏向工程集成（如 RAG-Anything 打包工具链）。总体而言，业界常用路径还是以能快速带来价值的方案为主：OpenAI 等用强模型结合简单检索，企业应用多用文本+OCR归一。而随着技术成熟，更复杂的融合也开始落地于高价值场景。下面我们将讨论这些实践中遇到的技术挑战，以及应对策略（核心问题5），继而展望未来发展方向（核心问题6的一部分）。



## 技术挑战与应对策略

实现一个多模态RAG系统并非易事。在将理论付诸实践的过程中，开发者会遇到许多独有的挑战，这是单模态RAG没有或者不明显的。总结来看，主要挑战包括**跨模态对齐、检索误差传 propagation、上下文整合、模型融合、效率成本、评估难题、安全考量**等。针对每点，我们分析问题并给出相应的策略或技巧。

1. **跨模态对齐与表示**：不同模态的信息天生“讲不同语言”，如何让机器理解一张图和一段文字在某种语义上是相关的？这涉及**多模态表征学习**的难题。如果选择路径一，需要训练或使用优秀的跨模态embedding模型确保图文共存一个向量空间 ；若是路径三，则融合模型需要能正确衡量图 vs 文的相关性。

   对此，目前的主流方案是利用**对比学习**进行跨模态对齐训练，例如 CLIP 就是通过让模型判别正确的图文对来学习共同表示空间 。策略: 借助预训练好的模型（CLIP、ALIGN、ImageBind 等）能加快开发。如果数据和需求特殊，可以对这些模型进行**微调**，例如使用领域内的图文对（医疗影像及报告）做对比学习，使embedding更贴合该领域语义。

   此外，在路径三的融合阶段，可以考虑用**注意力机制**让模型同时关注图像区域和文本片段，自动学习对齐。例如用 **ViLT**、**FLAVA** 这类模型的思路，将图像patch和文本token一起喂给Transformer，在中间层通过自注意力找到图文对应关系，然后再用于相关性判断 。总的来说，对齐问题需要结合模型训练和算法设计多管齐下解决，是多模态RAG的基础挑战之一。

2. **检索阶段误差与噪声**：RAG 系统有个固有风险：如果检索到的内容与问题不相关甚至有误，会直接影响生成答案质量，被称为“垃圾进，垃圾出”。在多模态RAG中，这种风险更复杂。

   一方面，文本检索一样可能取回无关段落；另一方面，图像检索/选择也可能失误。例如用户问一个产品瑕疵图片，系统却检索到外观相似但完全不同的问题图片——生成模型看错图，就会答非所问或张冠李戴。这些检索噪声会引入新的幻觉来源 。

   策略: 首先，尽量提高各模态检索的精度。可以使用多模态查询优化：比如结合文本和图像双模态一起过滤结果。如果用户问题本身包含图像和文字说明，我们可以先用文字搜，再筛选出含类似图像的那些结果，以减少图像误检。其次，引入重排序/校验步骤——即路径三思想——哪怕整体架构不是三，也可以在检索后用一个轻量多模态模型验证前N结果是否真的相关，不相关的就剔除 。

   例如，取回的每个图像我们都用GPT-4V快速判断一句“这图和问题相关吗？”，让它输出是/否。如果回答“不相关”，我们就摒弃该图。这样可以过滤明显跑偏的内容。再次，可以**设定模态置信度**：当检索结果相关性评分过低时（无论文本还是图像），宁可不使用该模态信息，以免干扰。比如找不到匹配图片，那就只给文字回答，不硬塞一张可能错误的图。通过这些手段，力求减少无关或误导内容进入生成环节。

3. **上下文整合与长度限制：多模态RAG带来的另一个难题是上下文变得更“大”更复杂**。在文本RAG里，我们已经担心选出的文本块太多超过模型上下文长度；现在还要加入图像。一个图像要么以二进制形式传给模型（如GPT-4V接受图像像一个特殊token序列），要么转成长篇描述文本。不管哪种方式，都**占用上下文容量**。同时，模型在整合图文时的顺序、格式也影响它理解。

   策略: 针对长度问题，可以**精选和精炼**上下文。例如，对长文本检索结果，我们可以先用摘要模型压缩，再提供给生成模型；对图像，我们可以给模型一个不那么详细但足够指导方向的描述，而不是像Caption模型那样面面俱到，这样减少字数。或者采取**分段对话**方式：先让模型看图做些分析输出，再把分析结果和别的文字一起喂入让它进一步回答。这类似链式思维，将大任务分拆，避免单次上下文太拥挤。GPT-4 系列模型有多达 32k token 容量，可以尽量利用，但其他模型上下文有限，必要时还是靠**截断与滑动窗口**技巧。还有一种有意思的方法是**图像不转文本**，而是交给模型视觉通道，这在GPT-4V中体现明显。它允许我们直接插入图像，而不像早期模型需要我们把图编成文字——相当于额外开辟了一条上下文“通道”。

   所以解决长度的关键一是压缩无用信息，二是善用多模态模型自己的输入结构来减轻文本膨胀。

4. **响应生成中的模态融合：假设检索阶段顺利，模型拿到了正确的文本段落和图像，但如何在回答中融合呈现这些信息**也是门学问。模型可能倾向于主要使用文本信息回答，因为语言对它来说更自然；也可能在有图像时忽略了部分文字依据，产生片面回答。这涉及提示设计和模型引导。

   策略: 我们可以在提示中明确要求模型结合多模态信息回答。例如系统消息写：“你会得到一些文本资料和一张相关图片，要参考它们给出答案。”提醒模型不要遗漏任何一方。此外，可以让模型在回答前**先输出一个中间模态理解**：比如先让它描述一下给的图像，然后再根据图像和文字回答问题。这类似 Chain-of-Thought，但应用在多模态上，确保模型对图有正确理解再回答。

   对于需要产出多模态响应（比如回答里想附图），提示也应指定格式，让模型知道可以嵌入图像引用。如果模型支持图文混合输出，可以引导它：“如果有相关图片，可以一并给出。”这些都属于**Prompt 工程**的方法。更高阶的做法是，**训练专用的融合模型/模块**。例如可以微调一个GPT-4V，让它在有文字+图像输入时，通过增加一个融合encoder来更好地综合信息，而不是简单地靠自注意力自己摸索。学术界有一些工作如 **Merlot Reserve**、**M3AE** 等，尝试让Transformer中专门学会视觉-语言交互，这些方法或许能集成到将来的大模型里，提升融合质量。

5. **效率与成本:** 构建多模态RAG往往意味着**更高的计算和存储开销**。比如路径二要存储每张图的文字描述，会比纯文本库膨胀不少。路径一要计算图片embedding，可能要GPU算，且图像向量比文字长，一次检索算力增加。路径三更不用说，要多次模型调用。还有模型推理层面，像 GPT-4V 这样的模型使用费用昂贵，每张图等于是额外收费的。对于需要实时处理大量请求的系统，这些成本都是实际问题。

   策略: 开源方法里，NVIDIA 和 Databricks 都强调了**尽可能预计算和缓存**  。比如可以离线完成对知识库里所有图像的OCR、embedding提取，甚至连摘要都提前算好存储。这样在线查询时，就不必再跑一遍大模型分析图像，只需做向量检索和取数据库内容即可，速度大大提升。其次，**尽量缩小需要调用大模型的范围**。如前述，可用小模型先做筛选、初步回答，然后大模型最后把关或补充细节。这类似多人协作：简单活让便宜模型干，疑难的交给昂贵模型。再次，可以考虑**模型蒸馏**：如果某些固定任务（如OCR后的校对、简单图像分类）GPT-4V 很擅长，可以用GPT-4V产生大量训练数据，训一个小模型专门执行，部署时用小模型处理大部分情况，只有遇到小模型不确定的时候再调用GPT-4V。这会极大节省API调用次数。工程上还可结合**批处理**（一次性给模型处理多张图，某些模型比如图像embedding可以打包计算）、**异步处理**（不阻塞主流程，比如让图像分析结果晚一点到也无妨，只要最终结合）等手段提高整体吞吐。最后，硬件上可投资高性能算力如GPU集群、使用向量加速数据库等，都是老生常谈的优化方向，不再赘述。

6. **评估与调试难度:** 多模态RAG 系统的评估是一个新挑战。文本RAG的评估已经不简单，要分别评估检索质量和答案质量；多模态情况多了图像等，评估维度更多。 指出，多模态RAG可能引入新的幻觉来源，因此评价需要既看检索到的材料相关度，也看生成答案正确性 。而相关度涉及跨模态，很难用传统信息检索指标衡量。

   策略: 有研究提出为多模态RAG设计专门的评估框架，如 RAG-Check 中定义了**相关度分（RS）和正确性分（CS）** 。RS由一个AI判别模型给出，评估检索出的图文是否真的有关；CS则评估最终回答与真实答案的符合度 。这些自动指标可以作为开发过程中比较不同模型或不同参数的参考。

   除此之外，**人工评测**仍不可少，特别是在高要求场景下，需要人来检查答案中的每个要点是否有依据、依据来自哪。如果目标是提供可查证回答（如一些系统会给出处引用），那更需验证引用指向的是否真能支持回答。这些最好由专业人员审阅一定样本量的结果来评判。

   同时，调试多模态RAG要采用**分模块排查**：先独立测试OCR模块正确率、图像caption质量，然后测试检索在每个模态上的准确率（比如给定问题能否找对相关图片/文本），最后再测试生成模型在完美检索输入下能否正确回答。这样分层诊断，可以定位瓶颈是在检索环节还是融合环节，再针对性改进。

   总之，评估多模态RAG需要结合**自动指标+人工评估+分层测试**，以逐步提高系统可靠性。

7. **安全与版权:** 最后，不容忽视的是**多模态内容带来的安全和合规问题**。文本模型输出不当言论、敏感信息一直是AI伦理焦点；加入图像等模态后，情况变得更复杂。模型可能根据图片生成侵犯隐私的描述（比如通过照片认出个人身份，这是**隐私问题**），也可能输出带有偏见的视觉描述（比如根据人长相妄测国籍健康，这是**AI偏见**）。另外，检索到的图像可能有版权或敏感内容，展示或使用需要谨慎。**策略:** 必须在系统中加入**多模态内容审核**机制。一方面，检索阶段对图片、视频内容做好过滤，例如敏感场景（暴力、色情）图片不作为候选，含人脸的图片要遵守隐私政策不能随意提供。同时，生成模型针对视觉内容的回答也要有限制，比如不得根据人物外貌猜测其种族、政治倾向等（这在许多AI准则中被禁止）以及不得对敏感图片给出详细描绘（如武器图纸等） 。可以通过在Prompt中加入这些**约束**说明，或在输出后用规则/分类器检测。如果系统会向用户展示检索图像，那也要考虑版权——一般可采用开源或授权图片，或在UI上提示图像来源版权信息。对于OCR提取的私人敏感信息（身份证照片等），要符合数据合规要求，可能需要对输出做遮盖或摘要而不泄露原文。总之，多模态RAG比文本RAG有更多的安全维度，开发者应参考已有的**多模态安全标准**（如微软给多模态AI设计的审核策略）来制定本系统的内容安全方案，确保既发挥多模态威力又不触碰法规与伦理红线。



综上所述，多模态RAG开发需要在算法和工程上平衡很多因素：既要信息尽可能丰富，又要防范噪声和风险。在应对这些挑战时，灵活运用技巧和工具，如预处理、轻量模型、提示工程、缓存、评估框架等，能够降低难度、提高成功率。本领域很多经验也在快速积累中，随着社区交流，我们有望获得更系统的方法论。



## 未来趋势

多模态RAG仍处在快速演进的阶段。展望未来，我们可以预见一些重要趋势和方向，将塑造这一领域的发展：



**1. 模型层面的融合与统一：目前，多模态RAG常常需要把不同组件拼在一起，例如单模态检索+多模态模型。未来的大模型可能朝着更加统一的多模态能力**发展，使这种拼接变得无缝甚至无需显式发生。OpenAI 已透露将来模型会更通用，Google 的 Gemini 被传是多模态模型，这意味着下一代基础模型可能**同时具备强语言和视觉能力**。如果我们有一个如 GPT-6 之类的模型，本身就能理解图文音视频并有巨大知识库，那RAG的形态可能简化为直接通过这个模型与一个多模态知识库交互就够了。甚至模型可以**自发执行检索**：像 ReAct/Toolformer 那样决定去查找某张图片或查询某段视频  。这种趋势下，RAG 系统架构会更简单，更多复杂度被模型的内部能力解决。当然，实现这一步需要AI模型有更高推理和自主性，目前还在探索阶段。



**2. 更丰富的模态扩展：今天多模态RAG讨论最多的是图像和文本，但未来音频、视频、3D**等都将纳入。音频RAG可以让AI听一段讲话录音然后查询背景资料回答问题；视频RAG让AI看 surveillance视频然后调取相关记录。随着传感器数据融入AI（IoT领域），可能还有红外图、深度图、地理信息等等。ImageBind等模型已经跨模态，这预示未来RAG**知识库会变成多元Modal**。例如一个企业知识库可能既有文本文档、图像库、录音档案、监控视频。当用户提问时，系统需要决定去哪个模态找答案，或者从各模态综合。例如问“去年大会上CEO的演讲主要传达了什么？”，AI也许会检索会议视频音频获得原话，再结合演讲稿文本来总结。目前这类跨视频音频的RAG还少见，但很有潜力。未来AI助理可能真的就像全能秘书，能读文件也能听录音、看录像帮你找信息。技术上，需要更好的音频转文本、视频理解模型，以及多模态embedding扩展到这些模态上。幸运的是，这些都在同步进步，如 Whisper 提供了强大的转录能力，可作为音频->文本工具接入RAG。



**3. 新的检索与存储技术：为了支撑多模态RAG，底层的向量数据库和检索算法也在演进。我们可能会看到专为多模态索引**设计的数据库，支持在同一个系统里管理多模态embedding，以便融合检索更方便。目前一些向量库（Milvus、Weaviate等）已经开始支持存储文本和图像两种向量，也提供metadata过滤支持模态标签。这趋势会继续，未来也许出现统一的“知识湖”（Knowledge Lake）容纳各种模态数据，提供统一查询接口。另外，**更高效的ANN算法**也在涌现，如基于 HNSW 的改进版，或者混合精确/近似搜索的方案，让在亿级向量上实时搜索成为可能。还有**交互式检索**值得期待，即模型和检索库可以多轮交互：第一次检索拿到一些结果后，模型分析说“这些图片不太对，我应该改个搜索词”，然后再搜。这其实在Bing等搜索引擎已经有雏形（用户反馈 refine），AI 代理可以自动做。这种闭环会提高检索召回率，也缩小候选集。也许不久的将来我们会给AI一个接口，它可以像人用谷歌一样，不停试不同查询直到满意——只不过查询对象不只是网页，还有数据库里的图片音频。存储层面，向量压缩、硬件加速（FPGA/ASIC）也会让多模态索引更轻便，能放下PB级数据，支持大规模企业部署。总之，检索技术升级是RAG进化不可或缺的一环。



**4. 回答形式多样化（生成多模态内容）:** 目前RAG系统的回答主要还是文本为主，有时附加一张图片链。但既然模型已经多模态，为何不让它直接**输出多模态结果**？未来的AI助手回答一个问题，可能是一段文字配两张图，甚至是一段自动剪辑的视频回答。这在科普、教育等场景很有价值。例如用户问“心脏如何泵血？”系统调取医学文本解释的同时，检索一段相关动画视频片段，一并给出。这需要生成式AI和检索更深地融合。一种可能路径是：先通过RAG找到素材（图像、视频、音频片段），然后用生成模型对这些素材做加工（比如拼接视频、加解说等）产出一个新的响应。这超出了传统NLP范畴，是多模态生成的新挑战。目前已经有一些 multimodal output 的尝试，例如 Stable Diffusion 可以根据文本和参考图像生成新图。如果把它应用到RAG，当用户问某历史场景，系统检索史料文字并找到相关绘画，然后令扩散模型根据文字描述生成场景图，让回答更生动。再如音频，AI可以生成一个语音回答，用一个合成声音讲解并配乐。可以想见，将来AI回答不再只是冷冰冰的一段话，而可能是图文音并茂的。这当然需要更复杂的流水线，以及在**生成内容真实性**上多加注意（生成的图也许不准确），但如果做好，是提升用户体验的革命性一步。对用户来说，他们希望的就是**更直观的答案**，多模态输出正合此道。



**5. AutoML 与 Agent 的集成:** 随着 RAG 系统复杂度提升，一个趋势是让AI本身参与到系统组建和调优中。所谓 **Agent** 指AI代理，可以看作一个智能 orchestrator。未来，也许我们的AI助手可以动态决定检索策略——这有点像AutoML思想，但应用在RAG上。比如模型发现自己文本没看懂，就主动要求“让我看看相关图片”；或它判断需要引用法律条文，就调用法条数据库。这种**工具使用**和 RAG 结合，将让系统更灵活。这和 ReAct 提到的让模型具备检索行动能力一致 。OpenAI 的 plan for tool use 以及 LangChain 的 Agent 模式都在往这方向努力。目前已经有Agent可以根据用户问题自动选择调用OCR、数据库查询或计算器等。加上多模态，这些Agent可以更智能：面对图文并茂的问题，它也能选对工具和模态。未来，也许我们配置RAG系统不再是写死流程，而是给AI一些可用动作，让它自己学习何时查图、何时查文，何时用哪个模型排序，真的成为一个“数字图书馆管理员”。这有助于针对不同问答需求实时优化流程，也可能减少人为硬编码，使系统更通用。不过Agent可靠性、决策正确性仍待提高，这是研究热点。



**6. 标准化与生态:** 随着越来越多应用出现，标准化会提上日程。比如是否会出现一个通用的多模态文档格式/协议，让不同来源的数据方便地被RAG系统摄取？像现在有HTML/Markdown定义文本格式，将来会不会有种新的格式同时包含文字、图、表结构化信息，AI 可以直接读这种格式理解文档各部分？又比如，多模态embedding有没有统一评测标准和开放基准？目前学术界开始提供一些多模态RAG benchmark ，未来可能演化出类似GLUE这样的权威评测，促进行业对标改进。在工具层面，LangChain、LlamaIndex之类框架肯定会继续进化，完善多模态支持（事实上它们已经在2023-24年加入了初步多模态模块）。未来开发者可能有更高抽象的API，比如 .add_image_index() 一行代码把图像库接入RAG，.query_all_modality() 自动处理跨模态检索，这将极大提高开发效率，推动更多创新应用诞生。同时，围绕多模态RAG的**社区和生态**也在形成，包括开源模型、数据集、教程等。可以预期不久的将来，会有更多现成的多模态知识库（比如开放的百科图像+文字嵌入库）供人直接使用。整个领域会更加繁荣和易用。



总的来说，未来的多模态RAG将向**更强的模型能力、更广的模态范围、更智能的检索方式和更丰富的输出**方向发展。这与通用人工智能的发展是一致的：让AI尽可能以多元方式感知世界并反馈给世界。对于研发人员而言，这既令人兴奋，又意味着需要持续学习跨越NLP、CV、IR（信息检索）多个领域的新知识。不过趋势是清晰的：**多模态RAG将成为新一代智能系统的基石**，支撑起真正懂文本、会看图、能听音的全能AI应用。



## 总结与建议

总结：多模态 RAG 将外部知识检索与多模态生成巧妙结合，赋予了 AI 系统前所未有的广度和深度。它的本质是在经典 RAG 框架中引入图像、音频、视频等多种信息源，扩充了模型可参考的“知识版图”，从而显著增强生成结果的真实性和丰富性 。我们分析了为何需要多模态 RAG：在信息形式日益多样的今天，仅靠文字难以覆盖全部知识，多模态手段可以捕获那些文本中没有或容易忽略的关键细节和语境 ；同时，通过参考视觉/听觉信息，模型的幻觉减少，对事实的把握更精确。这对于很多知识密集型任务来说是质的提升。



围绕实现路径，报告讨论了三种主流方案以及它们的原理、优劣势和适用场景  。我们看到，业界常用的是将图像等转为文本进行统一处理，或者利用统一嵌入模型实现跨模态检索，因为这些方法相对简单且验证有效  。更复杂的多模态融合排序在一些前沿应用中展露头角，但整体而言尚属少数。通过实际案例，我们了解了OpenAI 的 GPT-4V 如何成为多模态RAG的强力引擎，Meta 和开源社区如何提供各种模型和工具支持，Google 等如何把多模态检索融入产品，以及国内厂商的探索步伐。不同路径在性能、成本和复杂度上差异明显，我们在文中给出了详尽对比（核心问题6），帮助读者理解取舍。



技术落地过程中，我们强调既要有充足的深度（如模型结构、编码器、检索算法等技术细节的掌握），也要注意各种实用技巧和策略（核心问题5），如预处理优化、模型协同、提示设计、缓存加速等等。这些都是一线实践中宝贵的经验，能够决定系统成败。此外，我们不忘保持批判性视角，指出了当前多模态RAG潜在的问题和风险点，包括跨模态对齐难度、噪声干扰、效率权衡以及安全隐患等，并提供了应对建议。只有正视这些挑战，技术才能健康发展。



总的来说，多模态RAG正朝着更通用、更强大的方向迈进。它有望成为各行业 AI 应用的标配，从而极大拓展 AI 能帮助人类解决问题的范围。正如我们所展望的，未来的 AI 助手将不再“视而不见”或“充耳不闻”，而是真正进入“所见即所得，所问即所答”的境界——这其中，多模态RAG 功不可没。



建议：最后，我们面向对此领域有兴趣的读者（尤其是具备一定本科计算机/AI背景的工程师或研究者），提出几点建议：



1. **夯实基础，关注跨学科交叉：**多模态RAG涉及 NLP、CV、IR 多个领域基础。建议熟悉向量检索、Transformer 模型、卷积网络、OCR 原理等知识。本报告涉及的名词术语，诸如“embedding（嵌入）”、“CLIP 对比学习”、“OCR”、“多模态融合”等，都值得进一步阅读相关论文或教程以深入理解。在这个基础上，再关注领域交叉的新方法，如用于视觉的BERT、用于检索的对比学习等，加深对多模态任务特殊挑战的认识。
2. **从小规模原型入手：**构建一个完善的多模态RAG系统工程量较大，可以从小处着手逐步扩展。例如，可以先选用开源数据集（比如带图像说明的短文集），利用 LlamaIndex/LangChain 快速搭建一个简易多模态问答原型 。一开始也许只用路径二：图片先手工配点描述，然后看效果如何。逐步再引入CLIP等提高检索，最后接入GPT-4V等模型加强生成。通过原型迭代，您会更直观地体会每个组件的作用和问题，这比纸上谈兵收获更多。
3. **利用现有工具和服务：**善用业界和社区已有的工具链，可以事半功倍。例如文中提到的 Unstructured 库、Tesseract OCR、CLIP 模型、各大向量数据库、OpenAI/Cohere API 等，这些都是现成可用的。不必一开始就试图训练自己的多模态模型，那成本过高且意义不大。先用好别人的轮子，再视需要做微调或定制。比如如果做医疗文档问答，可以用现有OCR + BioBERT embedding + GPT-4V搭个demo，等验证可行后，再考虑要不要训练医疗专用embedding模型等进阶工作。
4. **重视数据和评估：**多模态系统开发中，“数据为王”依然适用。要投入时间准备和清洗高质量的数据，包括图像的正确标注、文字资料的准确分块等等。这直接决定检索和生成效果。同时，制定合理的评估指标和测试集，对开发过程中模型/系统的版本进行对比评测。比如您可以准备一批问题，让系统回答，然后人工给每个答案打分，看是否有根据地使用了图像信息、答案是否正确。有条件的话，引入如 BLEU/ROUGE 之类文本指标以及一些图文匹配指标综合评判。通过持续评测，找到系统薄弱环节进行加强，如发现OCR老出错就换引擎，发现模型老不看图就调整提示。数据和评估闭环能让开发少走弯路。
5. **关注最新研究与社区:** 多模态RAG是个前沿热点，新论文和新工具层出不穷。建议定期浏览相关领域的论文（例如本文提到的【25】号综述是很好的全面资料），以及关注 GitHub 上多模态项目动态。社区平台如知乎、Medium、Reddit 等常有专家分享实践经验和踩坑心得 。跟踪这些信息能帮助你获取更优方案和避免重复别人的错误。例如别人可能已证明某模型在某任务上效果不好，那我们就不必再踩雷。OpenAI、Google AI 等博客发布的新模型新能力也要留意，说不定下个月就推出一项把当前瓶颈迎刃而解的新技术。
6. **谨守伦理与隐私红线：**在推进技术的同时，务必时刻考虑安全合规。提前了解相关法律法规（如GDPR对生物特征数据处理的要求等）和行业规范（例如医疗数据隐私保护）。设计系统时内置必要的过滤和保护机制，哪怕在开发测试阶段也不要忽视（测试数据也可能包含敏感信息）。培养团队成员的伦理意识，确保大家上下一心既追求技术卓越也坚守责任底线。



多模态RAG 正以迅猛之势发展，它为我们描绘了智能助理的未来图景：那个助手可以读万卷书，览万张图，听万段音，还能举一反三、权衡佐证，为我们提供前所未有的知识服务。希望本报告的详尽分析能帮助您对这一领域有清晰的认识和实用的指导。在实践中不断学习和创新，您也有机会参与塑造这场 AI 变革，让人机知识交互更上一个台阶。祝您在多模态RAG之路上探索顺利，大有斩获！





**参考文献：**本文引用和参考了以下资料以支持观点：



- 【12】 Gautam Chutani, *“Multi-Modal RAG: A Practical Guide”*, *Medium*, Sep 17, 2024     .
- 【22】 NVIDIA 技术博客, *“多模态检索增强生成的简单介绍”*, Mar 20, 2024     .
- 【4】 NVIDIA Technical Blog, *“An Easy Introduction to Multimodal Retrieval-Augmented Generation”*, Mar 20, 2024  .
- 【13】 Databricks Blog, *“Unite your Patient’s Data with Multi-Modal RAG”*, Jun 6, 2025  .
- 【15】 Cohere Blog, *“Introducing multimodal Embed 3/4: Powering AI search & RAG”*, 2023  .
- 【16】 Jerry Liu et al., LlamaIndex Blog, *“Multi-Modal RAG”*, Nov 10, 2023     .
- 【2】 Shravan Kumar, *“Multimodal RAG with GPT-4-Vision and LangChain”*, Medium, Sep 4, 2024   .
- 【18】 Weizhe Lin et al., *“Retrieval Augmented VQA with Outside Knowledge”*, EMNLP 2022 .
- 【19】 Matin Mortaheb et al., *“RAG-Check: Evaluating Multimodal RAG Performance”*, arXiv Jan 2025  .
- 【25】 M.M. Abootorabi et al., *“Ask in Any Modality: A Comprehensive Survey on Multimodal RAG”*, arXiv Feb 2025   .
- 【26】 CSDN 博客, *“RAG是什么？RAG综述，看这一篇就够了！”*, May 22, 2025  .
- 【28】 Databricks 博客, *同上* (Healthcare场景)  .
