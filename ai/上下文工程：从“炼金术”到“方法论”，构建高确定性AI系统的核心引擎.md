# 上下文工程：从“炼金术”到“方法论”，构建高确定性AI系统的核心引擎

在大型语言模型（LLM）的实践中，我们常常陷入一种矛盾的狂热：一方面惊叹于它涌现出的强大能力，另一方面又对其输出的“不确定性”感到沮丧。我们像中世纪的炼金术士，反复调试提示词（Prompt），试图找到那组能点石成金的“咒语”。

然而，真正构建企业级、高可靠性的AI应用，靠的绝不是“炼金术”，而是一套严谨的“方法论”。**上下文工程（Context Engineering）**正是这套方法论的核心。它将我们的角色从“提示词魔术师”转变为“信息架构师”，其核心目标只有一个：**通过系统化地设计、输送和管理信息，最大限度地降低LLM推理的“认知负荷”，从而将不确定性转化为高确定性的输出。**

这篇文章将深入探讨上下文工程的本质，并为你铺开一条从理论到实践的清晰落地路径。

## 第 1 章 定义与边界：上下文工程究竟是什么、不是什么

### 定义

**主定义（面向工程落地）**

在给定任务与预算下，**选择、组织并按时送达**对模型最关键的**信息与工具**，使其在**可核验、可执行、可控**的前提下稳定完成工作。

- 选择：混合检索/过滤噪声，路由正确数据源；
- 组织：压缩/布局/结构化输出/引用锚点；
- 送达：在合适步骤注入上下文，必要时调用工具执行。

**等价视角 A（运维分解）**

CE = E + C + B&P

- E（Evidence Orchestration，证据调度）：混合召回→重排→压缩→前置布局，保证高信噪比与可核验；
- C（Contract to Action，行动契约）：结构化输出 + 工具/函数调用 + 引用对齐，保证“能落地、可追溯”；
- B&P（Budget & Permission）：token/延迟/成本与租户/版权/合规的护栏，保证生产可用。

**等价视角 B（形式化组合）**

Context = A(c_instr, c_know, c_tools, c_mem, c_state, c_query)

其中 `A` 为组装函数；`c_*` 为不同来源的上下文组件：

- `c_instr`：系统指令与规则；
- `c_know`：外部知识（RAG/知识图谱）；
- `c_tools`：工具与接口（函数调用）；
- `c_mem`：记忆（历史与长期存储）；
- `c_state`：动态状态（用户环境/多智能体状态）；
- `c_query`：用户当前请求。

Prompt 只是 `c_instr + c_query` 的子集，真正的 Context 是上述组件的**受控组合**。



### 为什么需要这门“工程”

1. 模型并不“过目不忘”。长上下文会出现**位置偏置**：对开头、结尾更敏感，中间最容易被忽略。把 100 页原文全塞进去，只会让关键信息**被噪声埋没**。

2. 企业问题强调**可核验与合规**：答案需要能指向证据，还要遵守权限边界。

3. **成本与延迟**是硬约束：token、Rerank、调用链都要钱、要时间。

   上下文工程解决的正是这三件事：**控噪声、保证据、守预算**。

### 与其它路线的边界（知道“不是什么”更重要）

- **RAG（检索增强生成）**：上下文工程的“骨干肌肉”。当你需要**最新/专有知识 + 可核验引用 + 权限**时，它是默认选项。
- **长上下文（Long Context）**：适合**单文档通读/风格保持**的场景，但别把它当“万能桶”；仍需**压缩与布局**避免中段丢失。
- **微调（Fine-tuning）**：当任务**格式稳定/风格固定/术语统一**，用小中模型微调可提稳健性与单位成本；但**知识频繁更新或需要证据**时，优先上下文工程，再视需要叠加微调。

### 三条设计铁律

#### 最小必要上下文（Least Necessary Context）

- **做法**：只放“对这次决策有直接贡献”的片段；其余一律留在检索层。

- **为什么**：上下文是“显存”，不是仓库。多一个无关段落，就多一份注意力被分走的可能。

- **例子**：问“**退货邮费谁承担？**”



- ✅ 放：退货条款里“**退货邮费由商家承担**”的句子 + 生效日期。
- ❌ 不放：运费险定义、仓库地址、与退货无关的“换货流程”。



#### 证据可核验（Citations First-Class）

- **做法**：答案必须能回指到**原文片段**或**外部工具的可追溯结果**；在生成前后做**引用对齐检查**。
- **为什么**：这是“可信”的唯一硬边界，能防止“看似对但无出处”的幻觉。
- **例子**：回答“退款时效 7 天”，旁边标注【来源：售后政策 §3.2，第 2 句】；若证据缺失则**拒答**或提示需要人工/外部系统确认。



#### 显式契约：结构化输出 + 工具调用

- **做法**：用 **JSON Schema** 限定输出；需要行动时走**函数/工具调用**（查库、跑 SQL、生成报表），模型只给参数。

- **为什么**：自由文本不可控、难重试；契约化让**自动化**成为可能。

- **例子**：

  - Schema：{"policy":"string","deadline":"date","is_refundable":"boolean","citations":[...]}
  - 如果要查“用户是否在可退期”，让模型调用 check_return_window(order_id)，而不是让它**猜**。



### 关键构件小词典

- **切块（Chunking）**：把文档按**语义/结构**切成小段，便于检索与引用。表格、代码要用**专门切块器**。

- **召回（Recall）**：从海量语料挑出**可能相关**的片段，拿到一个候选集合（Top-K）。

- **重排（Rerank）**：对候选集合逐段“深入比对”，把**看起来像**但**其实不相关**的剔除。

- **硬负例（Hard Negative）**：**表面相似但内容无关**的迷惑片段，会强烈干扰模型判断。

  例子

  - 问题：*“退货邮费谁承担？”*
  - 片段 A（正例）：“非质量问题退货，退货运费由消费者承担。”
  - 片段 B（**硬负例**）：“运费险由保险公司承担，理赔需在 48 小时内申请。”
  - 它们都出现“运费/承担”，但 B 讨论的是**保险赔付**，不是**退货运费**。把 B 也塞进上下文，模型极易得出“商家或保险公司承担”的错误结论。



- **HyDE**：让模型先写一段**“假答案的短文”**，拿它去向量检索，提高“问题太短/没标注”场景下的召回质量。

- **位置偏置（Lost-in-the-Middle）**：长上下文里，中间的信息最容易“糊掉”；把关键信息前置是常用对策。

- **结构化输出（Structured Outputs）**：用 Schema 让答案“对齐可解析格式”。

- **函数/工具调用（Function Calling / Tools）**：把“要做事”的部分交给外部系统执行；模型只负责“**决定做什么**”与“**填参数**”。

- **GraphRAG**：把文本抽成**实体-关系图**，先做“社区级摘要”，再结合原文检索，适合**多实体/多跳**问题。

- **Prompt Caching**：平台对**相同前缀内容**（系统指令/Schema/工具说明/示例）做缓存，重复使用更快更便宜——所以要把**稳定内容**放在提示的**最前**。

### 常见反模式

#### 反模式 A “把 Top-K 一味拉大”

- **现象**：从 10 拉到 50、100，觉得“多多益善”。

- **为什么坏**：K 变大，**硬负例**的比例陡增；模型注意力被分走，答案变得**似是而非**。

- **怎么改**：



1. 先**多路召回**（向量 + 关键词 + 多查询/HyDE），得到 30–50 个**候选**；
2. 用 **Rerank** 把候选压到 **Top-5/Top-8**；
3. 给每段证据打**一致性分**，低于阈值就不带进上下文。



- **例子**：问“退货邮费谁承担？”



- 拉 50 段：出现“运费险条款”“发货运费”“邮寄地址”等**硬负例**；
- 用 Rerank 压到 5 段：只剩“退货条款 + 生效时间 + 例外情况”，答案稳定。



#### 反模式 B｜“只用向量检索”

- **现象**：把 BM25（关键词）全关掉。
- **为什么坏**：**编号、术语、公式、代码符号**对语义向量并不敏感，容易漏召回。
- **怎么改**：**混合检索**（向量 + BM25）+ **融合排序**（如 RRF），保证“语义与字面”都照顾到。
- **例子**：问“**表 A 第 12 条**怎么规定？”只靠向量，很可能错过“第 12 条”的精确匹配。



#### 反模式 C｜“能塞就塞，整本手册进上下文”

- **现象**：害怕漏信息，把整篇放进去。

- **为什么坏**：触发**位置偏置**，中间信息易丢；同时**成本陡增**。

- **怎么改**：



- **预算化压缩**：先抽关键句；
- **布局优化**：把关键信息放在前部；
- **引用对齐**：保留原文片段 ID，答案里标注来源。



- **例子**：把“退货流程图”整页贴进去不如摘出“承运人遴选不影响邮费承担方”的一句关键话。



#### 反模式 D｜“自由文本输出，一把梭”

- **现象**：让模型写“自然语言长段落”，下游系统读不懂。

- **为什么坏**：不可解析、不可重试、不可审计。

- **怎么改**：**JSON Schema** + **函数调用**。

- **例子**：



- ❌ “您可以退货，邮费我司承担。”
- ✅ {"is_refundable":true,"postage_payer":"merchant","policy":"售后政策§3.2","citations":[{"doc":"policy_v7","sec":"3.2","sent":2}]}



#### 反模式 E｜“权限后补”

- **现象**：先检索、后过滤。
- **为什么坏**：即便最后没展示，**越权检索**也可能在日志里留下痕迹。
- **怎么改**：**检索前**按租户/角色做**索引级过滤**；**检索后**再做结果级过滤，双保险。



#### 反模式 F｜“不设预算，不看延迟”

- **现象**：为了“更准”，一路加模型、加上下文、加外部工具。
- **为什么坏**：等你“准”的时候，用户已经走了。
- **怎么改**：在 PRD 里就定下**P95 延迟/单位成本**，通过**压缩、缓存、模型级联**守住。



### 一个“带数字”的迷你案例（按 L1–L5 走一遍）

**问题**：*“7 天无理由退货，邮费谁承担？”*（SLO：P95 ≤ 2s；必须给出处；单位成本 ≤ X）

**Step 1｜L1：查询编排（≤ 80 ms）**

- 意图/参数抽取：{policy_domain: 售后, metric: 邮费承担, scope: 7天无理由}；
- 生成 `Q_norm` 与 3 条改写 + 1 条 HyDE 假文档；
- 判定无需分解（单跳问题）。若多跳问题则生成 `Q_graph`；
- 产出 `query_plan.json`（含约束与预算）。

**Step 2｜L2：多源召回与融合（≤ 300 ms）**

- 路由：向量库 + BM25；（无需 SQL/图谱）
- 并行检索：向量 Top-30 × 2（原问/HyDE）⊕ BM25 Top-30 ⊕ 改写 Top-10 × 3；
- 去重 + 融合排序（RRF）→ 候选 ≈ 50；
- 语义重排（cross-encoder）→ Top-6；
- 权限过滤：剔除跨租户/越权片段；
- 结果组成：退货条款 §3.2、例外（大件/生鲜）、生效时间等。

**Step 3｜L3：预算化压缩与布局（≤ 120 ms，预算 800 tokens）**

- LongLLMLingua：抽关键句、保留数词/否定/边界条件；
- 布局：承担方与例外前置；
- 附带引用锚点 {doc, sec, sent}。

**Step 4｜L4：生成→评审→行动（≤ 900 ms）**

- 第一次生成 Draft v1（严格 JSON Schema）；
- 自评（Self‑Refine/Reflexion）：检查引用一致性/字段完整性/否定词；
- 若用户给 `order_id`，调用 `check_return_window(order_id)`；
- 多评审（N‑CRITICS）融合，生成 Final。

输出示例：

```
{
  "is_refundable": true,
  "postage_payer": "consumer",
  "policy": "售后政策 v7 §3.2",
  "exceptions": ["质量问题由商家承担", "生鲜不适用7日"],
  "citations": [
    {"doc":"policy_v7","sec":"3.2","sent":2},
    {"doc":"policy_v7","sec":"3.5","sent":1}
  ]
}
```

**Step 5｜L5：记忆与会话治理（≤ 50 ms 额外）**

- 写入判定：无长期偏好/画像变化，不写长期记忆；
- 会话摘要：把“问答 + 引用锚点”写入短期摘要；
- 窗口治理：滚动 buffer 保留最近 N 轮；KV‑cache 采用 H2O 策略淘汰低重要性位；
- 观测：记录 Trace（召回/重排/压缩/生成/调用/引用）；指标：Recall@6、引用一致率、Schema 合规率、P95、单位成本。

> 用户展示由 JSON 二次渲染。政策更新仅需重建索引，无需改模型。



### 小结（把这一章落成“评审清单”）

- **定义**：选择 / 组织 / 按时送达 **关键信息与工具**——为**可核验、可执行、可控**服务。

- **铁律**：最小必要上下文｜证据可核验｜结构化输出 + 工具调用。

- **反模式**：Top-K 盲目增大｜只用向量不混检｜整本塞上下文｜自由文本无契约｜权限后补｜不设预算。

- **会做的三件事**：



1. 先把**证据**做对（混合召回 + 重排 + 压缩 + 布局）；
2. 再把**输出**做稳（Schema + 函数调用 + 引用对齐）；
3. 最后把**成本/延迟/权限**做实（预算 + 缓存 + 双层 ACL）。



## 第 2 章 分层架构与数据流：把上下文工程落成可复用的流水线

*——RAG 是骨干；检索→重排→压缩→编排→行动→观测与安全，一条跑通的工程链*



> 章导读：这一章不“堆知识点”，而是把上下文工程如何被**真正实现**讲清楚：一张**总览图**、一条**数据流**、五个**工程层**。其中把不那么家喻户晓的概念（LLMLingua/LongLLMLingua、ReAct、Toolformer、GraphRAG、HyDE、结构化输出/函数调用、Prompt Caching）逐个解释到能上手的程度。最后给一份**生产级强基线配方**（含参数与可操作检查单）。



### 总览图：系统是怎么跑起来的（先看图，再走路）



![RAG_pipeline](上下文工程：从“炼金术”到“方法论”，构建高确定性AI系统的核心引擎.assets/context_overview_L1-L5.svg)

**一句话把数据流说透**：

**把问题编排好（L1）→从多源高信噪召回并甄别证据（L2）→把材料变成“可被模型可靠吸收”的输入（L3）→让模型“按契约说话并能动手”（L4）→持续积累与治理记忆（L5）→全程可观测、可回滚、可控成本**。RAG 是**骨干肌肉**，L3 是**消化与摆盘**，L4 是**大脑带着手脚去干事**，L5 是**记忆与会话治理**，全程可观测、可回滚、可控成本为**神经系统与免疫系统**。



### L1｜摄取与索引（把料备对：可检索、可授权、可引用）



要点

- **清洗/切块/脱敏/加元数据**：切块不是均匀截断，而是**语义/结构感知**（段落、标题层级、表格单元、代码块）。每个块附：{title, section, time, doc_id, chunk_id, ACL}。
- **混合索引**：**向量库**负责语义相似，**BM25**负责编号/术语/符号等精确匹配。两条索引并存，为后续融合排序打基础。



常见坑

- *“只做向量”* → 漏召回专业编号/条款；
- *“切块太粗/太细”* → 粗了不准，细了上下文碎裂。**经验**：以“一个问句能被完整回答的最小段”为目标，表格/代码用专用切块器。



### L2｜召回与证据甄别（RAG 的骨干肌肉）

#### 路由与数据源选择（Agentic RAG 路由）

- **是什么**：基于 `Q_norm / Q_graph / C` 决定去哪里取证：向量库、BM25、SQL/数据仓库、知识图谱、Web/内部 API；可单源或多源并行。
- **怎么做**：轻量路由器（few-shot 规则 + 小模型分类）输出 `plan = [(source, operator, params)]`；对外部源设置**预算/速率/权限**护栏。
- **例子**：数值/聚合 → SQL 优先；强实体关系/路径问题 → 知识图谱/Graph；术语精确匹配 → BM25；开放域/冷门 → Web 搜索 + 向量。



#### 多路召回（Recall）

- **向量检索**：拿到语义相似的一批候选（Top-N）。

- **关键词检索（BM25）**：兜底**编号/术语/符号**类精确匹配。

- **多查询（Multi-Query）**：把一个问句改写成多种表达（同义/扩展）以扩大覆盖面。

- **HyDE（Hypothetical Document Embeddings）是什么？**

  让模型**先写一段“可能答案的短文”**（假文档），把它向量化去检索真文档。**用途**：冷启动/短问句/信息稀疏时，能更好“对上味儿”。





权限预过滤

- 在“提候选”的这一刻就按租户/角色进行**索引级过滤**；之后再做**结果级过滤**，双层兜底。



#### 图增强检索（Graph-RAG/知识图谱）

- **是什么**：从文档抽取实体-关系构成“局部知识图”，先在图上做社区/路径检索与摘要，再回到原文片段对齐引用。
- **为什么**：多实体多跳问题中，图结构能把“关系”显式化，抑制把不相关片段“拼贴”成幻觉的倾向；在生物医学/法律/制造等效果突出。
- **工程要点**：
  - 构图：实体抽取/消歧，边类型设计（因果、隶属、同义）；
  - 检索：社区发现（获取候选子图）+ 路径搜索（k-hop）+ 子图摘要；
  - 回对齐：子图节点附带文献证据锚点，便于 L3 引用对齐。



#### 语义重排（Rerank）

- **是什么**：用更强的句对模型对“问题 ↔ 候选段落”逐段打分，把“**看起来像但不相关**”的**硬负例**剔除。
- **为什么**：Top-N 出来的候选里总混着“预感很对却答非所问”的段落；**不重排就带毒**。
- **怎么做**：Top-N（30–50）→ Rerank → **Top-k（5–8）**；设置**一致性阈值**，不够就拒答/降级。



> **硬负例再解释（快速复习）**：
>
> 问“退货邮费谁承担？”——“运费险理赔规则”语义表面接近，但并不回答“退货运费”这个问题。把它带进上下文，模型很容易“张冠李戴”。**Rerank 的使命就是把这种“看起来像”的段落踢出去**。



### L3｜上下文构造：压缩、布局与编排（把料做成菜，易消化）

#### 预算化压缩

- **它是什么**：一种**以 token 预算为硬约束**的压缩方法，按重要性保留关键句/关键词/实体与数字，尽量不损失“能回答问题”的信息。

- **为什么不是“普通摘要”**：普通摘要追求可读性；**预算化压缩**追求“**在 X 个 token 里，最大化可用信息密度**”。

- **怎么用**：

  1. 给定预算（比如 800 tokens），
  2. 对 Top-k 证据段逐段压缩，
  3. 合并去重，
  4. 保留**引用锚点**（doc/chunk/sent_id）。



- **收益**：降延迟/降成本，同时**缓解长上下文的“中段遗失”（位置偏置）**。

- **形象比喻**：不是“写读书笔记”，而是“给开卷考试做**可查要点卡**”。

> 证据段
>
> 指**来自外部知识库**的、被检索系统选出来、并通过**重排（Rerank）确认“与当前问题最相关”的文本片段**（也可以是表格行/代码块）。
>
> - 典型字段：{doc_id, chunk_id, text, score, section, timestamp, acl}。
> - “Top-k”就是**重排后**按相关性得分取前 k 段（工程上常用 k=5~8），保证**小而精**，降低“硬负例”的干扰
>
> **对话历史**不是“证据段”。它是**会话态的上下文**，记录用户意图与上文决策——可以被**摘要**后放到提示前部，但它**不来自外部文档**，也不能当“可核验引用”。
>
> - 实操：把**对话摘要**作为“任务上下文”，把**Top-k 证据段**作为“可核验材料”；两者都进入 L3 的“上下文构造”，但用不同角色标注（conversation_context vs evidence）



#### 位置布局优化（为什么“放哪儿”很重要）

- **位置偏置**：模型对**开头/结尾**更敏感，中间更易“糊”。
- **策略**：把**答案关键句**、**限制条件/例外**、**数字/日期**放在上下文**前部**；把“背景描述/次要论据”往后排。

模型对于开头结尾更敏感的根据在于

- **Lost-in-the-Middle** 系列研究表明，主流大模型在长上下文里，对于**开头与结尾**的信息检索更稳，中间位置的关键信息命中率显著下降（“中段遗失”）。这在**多文档 QA**与**键值检索**任务上都被复现。
- **注意力与位置编码**：Transformer 的注意力分配与位置表征（如 RoPE/ALiBi）对**端点**信息更容易建立稳定对齐；
- **训练分布**：预训练语料里，开头常放“标题/摘要”，结尾有“总结/答案”，模型形成**端点偏好**；
- **稀释效应**：上下文越长，中段的关键信息被大量无关 token 包围，**信号-噪声比**下降。



#### 自我改进/评审回路与思维组织（先产出，再自评/多评审）

- **Self-Refine/Reflexion**：先生成“初稿”，随后触发自我评审提示（错误定位、缺证据、风格/Schema 违背），根据评审建议重写；将“初稿+评审”作为**新上下文**再加工，迭代 1–N 轮，直至满足“约束满足度与改进增益阈值”。
- **N-CRITICS/A2R**：引入多位“评论员”并行评审，从不同准则（忠实度、完整性、可执行性、风格）打分与提改进意见；通过**投票/加权融合**汇总，驱动最终修订。A2R 将评审→修订→再评审做成显式回路。
- **思维组织（CoT/ToT/GoT）**：
  - CoT：显式列出推理链，利于引用对齐与工具调用插入；
  - ToT/GoT：把问题拆为树/图状子任务，节点上挂“证据与动作”，对多跳、多约束问题更稳。
- **工程守护**：
  - 设定**预算护栏**（最大轮数、最大新增 token、P95 延迟上限）；
  - 评审提示实现“可控模板”，避免自由发散；
  - 将各轮中间产物（草稿/批注/评分）以**结构化槽位**缓存，便于追踪与回溯。



#### 编排为“可执行提示”

- **系统指令**：角色、语气、合规边界（例如“必须给出 JSON，若无证据则拒答”）。
- **证据块**：压缩后的段落 + 引用锚点。
- **输出契约**：**JSON Schema** 定义字段、类型、约束；
- **工具清单**：列出可调用的函数（名称、参数、返回结构、权限说明）。



### L4｜模型编排与行动：让答案“对齐 + 能干事”

#### 结构化输出（Structured Outputs）

- **是什么**：让模型严格按你提供的 **JSON Schema** 输出，**可解析、可重试、可校验**。
- **为什么**：自由文本好看，但**系统接不住**；Schema 让上下游对齐、错误可定位。



#### 函数/工具调用（Function Calling / Tools）

- **是什么**：模型不“自己做事”，它**提出要做的事与参数**；系统去执行（查库/跑 SQL/生成报表/调用搜索等），结果再回给模型。
- **好处**：可审计、可限权、可复现，避免“模型瞎编执行结果”。



#### ReAct & Toolformer（两种常被提到、但容易被说玄学的范式）

**ReAct（Reason + Act）**：**计划—行动—观察—再计划**的循环。模型先制定小计划（如“先查是否在 7 天内，再查邮费条款”），调用工具拿到结果，再继续下一步。

1. 定义可用工具：白名单 + 参数约束 + 超时（如 check_return_window(order_id)、lookup_policy(section)）。

2. 提示模版（放 L3）：



   ```json
   Thought: <你的思考>
   Action: <工具名>{"参数":...}
   Observation: <工具返回>
   ...（可重复若干次）
   Final: <最终JSON，必须符合Schema>
   ```



3. 执行控制（L4）：

  - 最多步数（如 ≤3）；
  - 只允许白名单工具；
  - 任一步失败→降级或拒答；
  - 每次 Observation 回灌后再让模型续步。

4. **终止**：当满足 Schema 且引用对齐通过时结束。



**迷你示例（“退货邮费谁承担？”）**

```json
Thought: 先确认是否仍在7天内，再查退货条款。
Action: check_return_window({"order_id":"A123"})
Observation: {"within_window": true}

Thought: 在退货政策中定位“邮费承担方”和“例外”。
Action: lookup_policy({"section":"return_policy"})
Observation: {"citations":[{"doc":"policy_v7","sec":"3.2","sent":2}, ...],
             "snippets":[
               "非质量退货，邮费由消费者承担。",
               "质量问题退货，邮费由商家承担。"
             ]}

Final: { ... 符合 JSON Schema 的最终答案 ... }
```





**Toolformer**：让模型在预训练式数据上**学会何时调用哪个工具**，像“带插件的大脑”。

- **工程落地怎么取舍**：
  - 需求**步骤清晰**、可枚举 → 直接**函数调用 + 明确调用顺序**；
  - 需求**探索性强/步骤动态** → 用 ReAct 做**轻规划**，但在生产要**限制可调用工具与参数范围**。



#### 引用对齐与一致性检查

生成完毕，逐条核对答案中的论断是否能在**证据块**里找到支撑；缺失则降级/拒答/提示“需人工/需工具进一步确认”。



### L5｜记忆与会话治理（Memory Orchestration）

> 让“有用的过去”在未来可被可靠调用，同时**控重复、控漂移、控成本**。

#### 记忆写入策略（何时固化为长期记忆）

- **触发阈值**：基于“**重要性/新颖性**”双阈值写入长期存储（语义/情节/Episodic）。
  - 重要性：与目标/KPI/长期偏好强相关（例如用户明确“永远不看 4K 视频”）。
  - 新颖性：与既有记忆差异显著（向量距离/关键词 Jaccard/实体变化）。
- **类型划分（CAIM 等认知式框架可借鉴）**：
  - 语义记忆（Semantic）：稳定偏好/事实/画像槽位，如 user_profile.interests、constraints；
  - 情节记忆（Episodic）：“谁-在何时-做了何事”的事件记录，含来源与置信度；
  - 程序性/技能（Procedural）：可复用的操作配方/工具调用宏。
- **写前审查（Gate）**：
  - 冲突检测：与现有槽位对齐校验（单位/时间/否定词）；
  - 敏感过滤：隐私/权限标签继承；
  - 配额约束：每会话/每日写入上限，避免“过拟合当前对话”。
- **写入形态**：统一为结构化对象，便于后续合并与检索，例如：

```
{
  "type": "episodic",
  "who": "user:123",
  "when": "2025-08-23T12:30:00Z",
  "what": "取消订阅因价格上涨",
  "evidence": [{"doc":"chat","turn":42}],
  "score": {"importance":0.82,"novelty":0.76},
  "provenance": "dialogue"
}
```

#### 去重 / 合并 / 冲突消解（反思式记忆管理）

- **近重复检测**：语义相似度 + 规则（同实体/同槽位/时间接近）→ 合并为单条，累积置信度与计数。
- **冲突消解**：
  - 信任评分：按来源可信度、时间新近性、引用数量加权；
  - 策略：最新优先/证据优先/人工仲裁；
  - 保留多版本与溯源，不“硬覆盖”。
- **再巩固（Reconsolidation）**：定期批处理“反思摘要”，把多条事件凝练为高层规则（例如“在商务旅行期间更偏好直飞”），并回写到语义槽位，防止**画像漂移**与“碎片化饱和”。

#### 记忆读取策略（把长期存储拼回当前上下文）

- **按槽位检索**：
  - 谁（Who）：实体画像与关系（用户/同事/供应商）；
  - 何时（When）：按时间窗/节律（工作日/节假日）；
  - 哪件事（What/Which Event）：按事件类型/主题词。
- **结构化拼装**：读取结果按固定槽位回填到提示前部：

```
memory_context = {
  "user_profile": {...},
  "session_goals": [...],
  "episodic_recent": [{who, when, what, evidence}],
  "constraints": {budget, policy, privacy}
}
```

- **容量治理**：
  - 基于任务相关性与新近性排序，设最大 token 配额；
  - 关键锚点（长期偏好/禁令）固定前置；
  - 低相关项转为短摘要或仅保留引用锚点。

#### 缓存与窗口治理（Rolling Buffer 与 KV-Cache）

- **滚动对话缓冲（Short-Term）**：维护短期对话窗口（如最近 N 轮），配合“**会话摘要器**”定期把历史压成“任务上下文”，降低窗口膨胀。
- **KV-Cache 管理（Model-Level）**：
  - StreamingLLM：流式维护注意力状态，降低长会话延迟；
  - **H2O**：对 KV-Cache 做重要性/新近性/位置混合淘汰（如保留开头锚点、关键词密集段），控显存；
  - 与“关键句前置/锚点固定”策略一致，避免把“噪声历史”永久留在注意力里。
- **超长会话策略**：分段归档 + 章节化索引；必要时“重开新会话 + 引用历史摘要”，在**可用信息**与**算力/延迟**之间取得平衡。



### 观测、安全与成本工程（跑得快也要看得见）

**双层权限**



- **检索前**按 ACL 做索引/查询过滤；**检索后**再做结果级过滤，防越权泄露。



**可观测**

- 为每次回答记录**查询→召回→重排→压缩→编排→输出/调用→引用**的 Trace；
- 关键指标：**Recall@k、Rerank 提升Δ、忠实度、引用一致率、Schema 合规率、P95 延迟、单位成本、拒答率**。



**成本与 SLO**

- **Prompt/Context Caching**：把**稳定前缀**（系统指令/Schema/工具说明/示例）放在提示最前，提升缓存命中，显著降时延与成本。
- **模型级联**：轻模型做召回/重排，重模型做综合；
- **熔断与回滚**：指标异常自动退回“安全基线”（例如禁用压缩或禁用某个工具调用路径）。



### 生产级“强基线配方”（能直接照着搭）

> 目标：以**稳定正确/低延迟/可核验**为第一优先，先把“骨干肌肉”练强，再加花活。



**输入**：问句 Q（可含业务上下文/用户身份），预算 800–1,200 tokens，P95 ≤ 2s（示例），必须输出 JSON。

**Step-by-Step**



1. **摄取与索引（离线）**
  - 语义/结构切块：段落 120–300 token；表格按行/单元格；代码按函数/文件块。
  - 元数据：{doc_id, chunk_id, title, section, time, acl}。
  - 建双索引：向量库（HNSW/IVF 等） + BM25。



2. **查询编排（在线，轻量）**
  - 解析意图/实体/时间与约束，形成 `Q_norm`；
  - 是否需要**问题分解**：生成 `Q_graph` 与聚合契约；
  - 生成多查询改写与 HyDE 假文档；产出 `query_plan.json`（供召回使用）。



3. **召回（在线）**
  - 路由器选择数据源：向量/BM25/SQL/图谱/搜索/API（可多源并行，带预算/权限约束）。
  - 向量 Top-30 ⊕ 关键词 Top-30 ⊕ **HyDE** Top-10（按 query rewrites 并行检索）→ **去重 + 融合排序（RRF）** ≈ Top-50。
  - 若命中“图通路”，执行子图检索/社区摘要并回对齐原文片段；
  - 做**索引级权限过滤**（租户/角色）。



4. **语义重排**
  - Cross-Encoder / Rerank → **Top-k=5~8**，并为每段生成**一致性分**；低于阈值（如 0.6）剔除。
  - **注意**：不要“Top-k 盲目拉大”，硬负例的比例会随 k 上升而上升（上一章已用例子解释）。



5. **预算化压缩与布局**
  - LongLLMLingua：把 Top-k 压到预算内；保留数字、日期、否定词与边界条件。
  - 布局：**关键句前置**，背景次序靠后。
  - 为每段保留引用锚点 {doc/chunk/sent}。



6. **编排提示**
  - System：角色/合规/必须 JSON/必须引用。
  - Context：压缩证据块（带锚点）。
  - Tools：允许调用的函数（白名单与参数约束）。
  - Output：**JSON Schema**（严格模式）。



7. **生成与行动（含自评/多评审）**
  - 结构化输出 → 若需查实时信息（如“是否仍在 7 天内”），调用 check_return_window(order_id)；
  - 合并工具返回，第二次生成最终 JSON。



8. **记忆与会话治理（L5）**
  - 写入策略：重要性/新颖性阈值；
  - 去重/合并/冲突消解与再巩固；
  - 滚动 buffer 与 KV‑cache（StreamingLLM/H2O）；



9. **引用对齐与校验与观测**

   检查 JSON 中的每个关键断言是否在证据块中找到对应句子；缺失则降级/拒答。



8. **观测与缓存**
  - 记录全链路 Trace；**开启 Prompt/Context Caching**；
  - 看板监控：ITR、忠实度、引用一致率、P95、单位成本、缓存命中率、拒答率。



**落地检查单（做完以上 9 步逐条自查）**



- 同问句多次调用，Top-k 组成稳定、答案可复现
- JSON 严格符合 Schema，异常可重试
- 每条答案都有可回溯引用；缺证据时系统会拒答
- P95 与单位成本在预算内；缓存命中率达标
- 权限在检索前/后均生效；越权检索零容忍
- 可一键回滚到“无压缩/无工具调用”的安全基线



### 概念速通卡（本章出现的“容易陌生”的名词，30 秒回忆）

- **HyDE**：先写“假答案短文”，拿它做向量检索——冷启动时救命的召回放大器。
- **Rerank**：用强模型逐段打分，把**硬负例**踢出去（Top-N→Top-k）。
- **硬负例**：表面相似、实则不答题的段落（运费险≠退货邮费）。
- **LLMLingua/LongLLMLingua**：**按预算**压缩证据，保关键句与实体，兼顾速度/成本/正确性。
- **结构化输出**：按 **JSON Schema** 输出，机器能读、能重试、能校验。
- **函数/工具调用**：模型只给“要做的事与参数”，系统去做，结果回填。
- **ReAct**：计划→行动→观察→再计划；步骤不固定的任务用它更稳。
- **Toolformer**：让模型学会**何时**调用**哪个**工具（更“本能”的工具选择）。
- **GraphRAG**：把文本变“实体-关系图”，做社区摘要+原文证据的混合检索，适合多实体多跳。
- **Prompt/Context Caching**：把**稳定前缀**放前面，平台自动缓存，延迟和成本一起下台阶。

### 小结（本章你真正带走的东西）

-  一条可执行的数据流：**L1 问题编排与规范化 → L2 多源召回与甄别（路由/融合/图检索）→ L3 做成菜（压缩/布局/编排）→ L4 大脑+手脚（结构化输出/规划/工具调用）→ L5 记忆与会话治理**；全程配以**观测/安全/成本护栏**。
- 把**陌生名词**讲到能用：LLMLingua/LongLLMLingua、HyDE、ReAct、Toolformer、GraphRAG、结构化输出、Prompt Caching。
- 一套**生产级强基线配方**：你可以直接照着搭，先把骨干肌肉练强，再追求更花的增强（比如 GraphRAG、智能规划、多工具协同）。



## 结语

从“大海捞针”式的提示词调试，到构建一个精密的、自动化的信息处理流水线——这就是上下文工程带来的认知飞跃。它虽然复杂，但它将AI应用开发从一门“艺术”变成了一门真正的“工程科学”。

放弃寻找“完美提示词”的幻想吧。真正的护城河，在于你如何为你的大模型构建一个无与伦比的、高效的信息供给系统。这，就是上下文工程的全部意义所在，也是通往通用、可靠人工智能的必由之路。


