# 				提示词工程技巧实战指南

在与大型语言模型交互时，如何设计有效的提示词（Prompt）直接决定了解决方案的质量。本指南将从基础、进阶到高阶三个层次介绍提示词工程的实战技巧，并配以代码、写作、问题排查等案例说明。我们将特别关注近年来（2022–2025）的技巧演变，指出哪些老方法效果下降，哪些新方法成为最佳实践。

## 基础技巧

### 明确任务目标与约束

与其给模型一个含糊请求，不如清晰具体地描述你的目标、要求和约束条件。例如，不要只说“帮我写点东西”，而是具体说明**写什么**、**多长**、**什么风格**等。清晰的描述能显著降低模型理解歧义，强化任务目标，并控制输出风格和格式。研究表明，在提示结尾明确提出主要问题，可以帮助模型聚焦关键信息，从而提高回答质量（Anthropic 的实验显示复杂场景下质量提升可达 30%）。

**提示要点：**使用明确的动词和肯定式指令，少用模糊词语或双重否定。提供必要的背景信息（如业务上下文、输入数据）帮助模型理解你的意图，但要注意相关性和简洁。如果背景太长，考虑使用要点列表或摘要来结构化呈现。例如，与其提供整篇文档，不如提炼出关键条目供模型参考。

**适用场景：任何任务都应遵循清晰明确的描述，尤其是需求复杂或结果要求严格的场景**（如代码编写**需要指明语言/性能要求**，文章撰写**需限定字数/风格**等）。对于简单问答任务，清晰提问同样有助于避免跑题。

**可能过时：早期一些使用GPT-3的经验认为模型只需非常简短的指令即可发挥想象，但面对GPT-4等强大模型，冗长但不相关的说明反而可能干扰其判断。如今更强调高质量信息**而非堆砌大量提示，避免给模型过多无关背景。

### 四段式提示结构：角色 + 背景 + 任务 + 输出格式

实践中，一个**“角色、背景、任务、格式”**四段组成的提示模板非常有效。这四要素分别为：

- **角色(Persona)：**赋予模型一个身份或角色定位。例如“你是一名资深Go工程师”或“假设你是法律顾问”等。不同角色会影响模型回答的风格和侧重点。在需要特定专业知识或语气时，设定角色能使回答更符合预期。
- **背景(Context)：**提供任务相关的上下文、资料或场景设定，帮助模型更精确地理解问题。例如提供业务需求、输入数据格式、已知信息等。但要确保背景简洁且紧扣任务，避免模型迷失在无关信息中。
- **任务(Task)：**清晰描述要求模型完成的具体任务。本段是提示的核心指令，要直接了当地说明要做什么。例如“请实现一个支持 1000 QPS 的高并发HTTP服务”。将主要指令放在提示最后，以确保模型着重关注。
- **输出格式(Format)：**指明希望模型产出结果的形式或风格。例如要求输出Markdown列表、JSON结构、代码片段或限定字数等。明确的格式要求能减少后续整理工作，并提高输出的一致性。

下面通过一个代码场景示例，比较“四段式”提示与普通提示的效果：

```
❌ 不佳提示：
“帮我写一个高并发HTTP服务的代码。”

✅ 改进提示：
你是一名有5年经验的Go语言后端工程师。
背景：我们正在开发一个Web应用，需要一个高并发的HTTP服务来处理用户请求。目标QPS（每秒请求数）需要达到1000。
任务：请用Go语言编写上述HTTP服务的完整代码，确保代码具有良好的并发性能。
输出格式：提供可运行的Go代码，并包含必要的注释以解释关键实现细节。
```

在不佳提示中，模型只知道“写HTTP服务代码”，可能会困惑于细节，不确定性能要求或输出形式。而改进提示通过**角色**限定了专业角度，提供了**背景**需求（高并发1000 QPS），明确**任务**是编写代码，并指定**输出格式**为可运行且附注释的完整代码。这将大幅提高模型输出的针对性和完整性。

**适用场景：**四段式结构几乎适用于所有复杂任务提示，特别是**软件开发、长文创作、专业领域问答**等需要指定语境和风格的场景。例如让模型扮演医生回答医学问题，或者提供产品背景让模型撰写营销文案。对于简单任务（如基本算术）则无须面面俱到设定角色和背景。

**可能过时：**四段式结构本身是稳健的提示设计框架，在2023–2025年被各大团队视为最佳实践。需要注意的是，不要为了套用模板而加入无关紧要的“背景”或假扮不必要的“角色”。早期有人喜欢给模型长篇的人设故事，但如果任务非常客观直接，例如提取简单数据，过度角色扮演可能无益。**总之，信息要素齐全但不啰嗦**，这一定律在模型迭代中始终有效。

## 进阶技巧

### 多轮对话改写与迭代提升

与其期待模型在单次回答中完美解决复杂任务，不如采用**多轮对话迭代**的方法：**先让模型给出初稿**，然后再根据需要**要求改进**。这种“**草稿 -> 反馈 -> 精修**”的链式提示被证明比一股脑的长提示效果更好。研究对比了单轮“大杂烩”提示和拆分为“起草-评估-改进”三个提示的差异，发现分步提示的结果质量提升了约20%。原因在于，一次性要求模型执行多个复杂任务时，模型可能会因为“知道后面还会有修改”而在第一步就敷衍输出粗糙草稿。相反，**每轮专注一个子任务**能让模型更聚焦，逐步逼近高质量答案。

实际应用中，可以按以下模式展开：

1. **提出子任务1并获取初稿：**例如调试代码时，第一步先让模型**找出代码中的错误**。模型给出标注错误的代码和解释。
2. **提出子任务2针对性改进：**接着要求模型**修复这些错误**并提供优化版本。模型据此输出更正确的代码。
3. **提出子任务3进一步提升：**如果还需要，可以再要求**优化性能**或**添加注释**等。逐轮完善直到满意为止。

这种多轮策略在**内容创作**上也适用：先让模型列大纲，再充实细节，最后润色语言。逐步引导能防止一次性生成时结构混乱或偏题。**一个经验法则**是：如果你的要求中包含超过一个动词（表示多个动作），那就可以拆分成多个Prompt逐一完成。

> 💡 **案例：**用户最初请求：“请帮我修正这段代码的错误并优化性能。” 这是两个要求。改进做法是先让模型“找出并修正代码错误”，待模型输出修复后的代码后，再让它“基于修复后的代码进行性能优化”。分两步完成，模型在各阶段都会更专注，结果也更可靠。

**适用场景：**当**任务复杂或具有多步骤**时（如调试并优化代码、撰写并润色长文、先分析再决策等），多轮迭代能显著提升效果。也适用于追求高准确性的场景——先让模型给出草稿答案，再通过追问细节、让模型自我检查来提高正确率。GPT-4等模型擅长保持上下文，能在多轮对话中记住先前输出，逐步改进。

**可能过时：**过去模型能力有限时，人们倾向于在单个Prompt里交代尽可能多的细节，试图“一步到位”得到答案。然而2024年以来，随着研究者深入探索，这种**将多个要求塞进一条Prompt**的方法被发现有时适得其反。现在更强调**Prompt要职责单一**，一步只做一件事，然后串联多步完成整体任务。如果仍沿用老思路写长提示试图一步完成所有步骤，反而可能因为指令混杂使模型无所适从。

### 思维链提示（Chain-of-Thought）与分步推理

**思维链（CoT）**技巧指引模型**一步步推理**，将复杂问题拆解为多个推理步骤。它在数学推算、逻辑推理等需要严谨多步思考的任务上非常有效。例如在数学题中要求模型“请一步一步推理再给出答案”，往往能减少模型跳步导致的错误。让模型显式列出推理过程，可以降低幻觉或不合理结论的概率。即使在2025年，CoT依然是提高复杂推理准确度的有力工具。

然而，**并非所有场景都需要显式的思维链提示**。对于**简单任务或基础写作**，模型无需逐步解释即可直接给出结果。在这些情况下强行要求“逐步思考”可能让回答变得啰嗦、不自然。例如，你让模型写一封日常邮件，却提示它“分步骤思考”，它可能会真的列出1、2、3步的思考过程，反而偏离了你想要的成品。在**最新的推理型模型**（如GPT-4）上，过度使用CoT技巧也可能是多余的——这些模型内置了强大的推理能力，往往不需要特别提示也会在内部进行推理。所以，Google 的提示工程指南建议：**在使用专门强化了推理能力的模型时，不要滥用“让我们一步步思考”的提示**。

> 💡 **案例：**对于一道复杂的逻辑谜题或数学应用题，在Prompt中加入“请分步骤推理并给出答案”有助于模型梳理过程并得出正确结论。但对于像“今天天气怎么样？”这样的简单询问，就不需要思维链提示。

**适用场景：****数学计算、逻辑推理、代码审查、复杂决策**等需要多步推理的任务，非常适合CoT提示。特别是在**考试解题、法律推理**这类需要模型逐条论证的情境下，要求模型输出推理过程能让答案更可靠。在使用规模较小或较旧的模型时，CoT提示也常能补足其推理短板。

**可能过时：**思维链提示在2022年因“Let’s think step by step”而走红，但到了2025年我们发现其局限性：对已经很强的模型来说，一味在所有Prompt后附加“请一步一步思考”不再是灵丹妙药。这种机械式提示有时会让模型**过度逐字输出推理**，增加冗余信息。如今更稳定的方法是引导模型**先给出答案草稿，再让它检查推理**，而非仅靠一句魔法咒语式的提示。从业者认识到新技术如**树状思维 (Tree-of-Thoughts)、ReAct、Self-Reflection**等在某些任务上表现更佳。因此，“一步步思考”提示不应滥用，而应根据任务类型权衡使用，在简单任务上可以省略，在复杂任务上可以与其他技巧结合使用。

### Few-Shot 示例提示与上下文学习

大型模型具备强大的**上下文学习**(In-context Learning)能力，会从提供的示例中学习模式并应用于新请求。这催生了**Few-Shot Prompting**（少样本提示）策略：在正式提问前，先提供**1个或多个示例**示范期望的回答格式或风格，再让模型回答新问题。相比之下，**Zero-Shot Prompting**则不提供任何示例，直接让模型凭借训练中学到的知识完成任务。

举例来说，如果要模型将一句话翻译成法语：

- **Zero-Shot：**你直接提示“请将‘I am learning Python.’翻译成法语。” 模型会依赖其已学知识直接回答。这通常能行得通。
- **Few-Shot：**你先提供类似示例，例如：
  *示例1:* “I like basketball.” → “J’aime le basket.”
  *示例2:* “This is a pen.” → “C’est un stylo.”
  然后再提问：“请翻译‘I am learning Python.’ → ”。有了前面的参考，模型更清楚输出要用法语，格式上保持一样。

Few-shot 提示的价值在于**引导模型遵循特定模式**。这在要求输出**特定格式**（比如回答要严格用“正面/负面”二选一）、**特定风格**（比如幽默的语气）、**或者复杂任务**时特别有用。通过一两个高质量示例，模型往往就能模仿出我们想要的调性和结构。

需要注意Few-shot示例的**选择和数量**：通常1~4个示例就足以让模型领会需求，再多增加样本往往报酬递减。而且示例过多会占用上下文长度，对于长文本模型可能会导致后面的指令被截断或忽略。示例顺序也可能影响输出，通常按从简单到复杂排序更容易让模型理解模式。另外请确保示例本身**完全正确**且**紧贴需求**——错误的示例只会让模型学到错误的东西。

**适用场景：**当**模型零样本表现不稳定**或**输出格式要求严格**时使用Few-shot最佳。例如：情感分类需要模型只回答Positive/Negative，用几个已标注的例子效果会更好；又如要求模型以某种文风写作，提供一段范文示例能让输出风格更统一。对于GPT-4这类强模型，很多常规任务零样本已足够，但在**小样本数据任务、格式敏感输出**（代码格式、日志格式）以及**风格迁移**（仿写某作者文风）上Few-shot仍大有帮助。

**可能过时：**在GPT-3时代，Few-shot曾是提升准确度的主要手段。但进入GPT-4时代，模型泛化能力增强，许多场合零样本就能给出不错回答，少样本技巧的重要性相对降低。不过**In-Context Learning**仍是2024–2025模型的一项核心能力，对于定制输出格式和风格依然有效。需要避免的是过去有人滥用很多无关示例企图“砸”出正确答案的做法——过长的提示可能触发上下文窗口上限，反而让模型忽略真正的问题。总之，示例要少而精，确保模型专注学习所需的模式。

## 高阶技巧

### 多角色协作提示

当一个问题涉及**多方面视角**或需要严谨校验时，可以引入**多角色协作**的提示策略：让模型同时或分步骤模拟多个专家角色，各自思考后再综合结论。这种方法利用了模型强大的角色扮演和对话能力，让不同“人格”在脑海中辩论，从而发现单一视角下可能遗漏的点。

**如何操作？** 可以在提示中显式指定多个角色，让模型依次给出每个角色的观点。例如：*“现在分别以‘代码审查员’和‘系统架构师’的身份对以下设计方案提出看法，然后综合两者意见给出最终改进方案。”* 模型会先生成代码审查员视角的分析，再生成架构师视角的分析，最后整合成统一的建议。这类似于让模型开一场内部会议，不同专家各抒己见，然后达成共识。

> 💡 **案例：**对于一个复杂的软件设计，你可以提示：“专家A（注重代码质量）认为：… 专家B（注重扩展架构）认为：… 请综合上述专家意见，给出最终方案。” 模型将输出A和B的观点以及一个融合两者优点的方案。又比如法律领域，让模型分别扮演原告律师和被告律师各自论证，最后以法官身份总结裁决。这样的提示能帮助**审视问题的不同侧面**，提高答案全面性。

**适用场景：****架构设计、策略制定、辩论分析、创意脑暴**等需要多角度权衡的任务，非常适合多角色提示。在代码领域，可以让“代码生成者”和“代码审查者”角色先后出现，以自动检查代码漏洞。在文章写作中，也可以让模型先作为批判者挑错再作为作者修改。Anthropic等公司报告指出，在组织决策情景下，引入多角色的讨论能生成更全面的分析。

**可能过时：**多角色提示是近一两年兴起的新技巧，之前的模型往往难以在一个回答中平衡多个 persona。GPT-4 的对话能力让这种技巧成为可能。但也要避免过度角色设定导致的输出混乱。如果任务本身**客观明确**（如纯计算或查找），引入多个虚拟角色可能徒增复杂性。另外，以前一些“让模型自问自答”的简单套路较初级，如今更强调**精心设计角色**使之互补，而非为了角色而角色。因此请确保每个引入的角色都有明确的用途，不然就可能回到过时的“无效角色扮演”窘境。

### 对抗性提示与压力测试

在模型投入实际应用前，设计**对抗性提示**对其进行压力测试，可以发现模型容易**幻觉**（瞎编事实）或**逻辑漏洞**的边界情况。思路是**刻意为难模型**，看看它如何应对，从而改进我们的提示或筛除风险。

**方法1：诱导幻觉测试。**提出一些*超出模型知识范围*的问题，观察模型是否不懂装懂。例如：“请问著名科技公司 X 在1823年发生了什么？”——真实情况是1823年可能该公司还不存在，人类会回答“不可能发生，因为那时没有X”。如果模型却给出一段貌似合理但实为杜撰的历史，那就暴露了幻觉倾向。通过此类提问，你可以了解模型在知识真空下会如何反应，并相应地调整提示（比如要求它不确实就回答不知道）。

**方法2：逻辑一致性测试.**提同一问题的不同表述或提供矛盾信息，看模型回答是否前后一致。比如先问：“如果我用电吹风给冰块加热，会发生什么？”再改问：“冰块遇热会冷却吗？解释原因。” 合理的模型应当前后一致地说冰会融化而非冷却。如果模型出现自相矛盾，你需要在提示中加强逻辑约束，或分步引导它推理。

**方法3：自我检视提示.**让模型**扮演审查者**来检查自己的回答。例如，在模型给出一段分析后，追问：“请以严格挑剔的科学记者身份，检查以上回答中有无未经证实的断言或逻辑错误，并指出依据。” 这种让模型充当“对手”的提示，可以经常发现原回答中的盲点和幻觉部分。很多时候，模型在这个自我批评环节会识别出之前不合理的内容，然后你可以让它基于这些反馈修改答案。这类似于Anthropic提倡的“让AI自己指出可能的错误并更正”方法。

**适用场景：**对**高可靠性要求**的应用（如医疗解答、法律咨询、金融建议）尤其需要对抗性提示来验证模型输出的可信度。在**生成代码**时，也可让模型尝试恶意输入（如极端数据）来测试代码健壮性。在提示工程研发阶段，用对抗提问来发现模型薄弱环节，能帮助我们改进提示或增加防范。

**可能过时：**早期对抗测试更多是人工完成，但现在已融入提示工程流程。值得注意的是，一些旧的简单对抗技巧（如在Prompt里直接命令“不要编造任何东西”）对2025年的模型来说作用有限——顶尖模型往往声称不会编造但还是可能产生幻觉。因此新的实践侧重于**诱导模型表现问题**而非仅口头警告模型。未来模型变得更透明可靠时，这种穷尽反例测试的重要性可能下降，但在2025年，系统化的对抗性提示仍是保证输出质量的一大利器。

### 要求结构化输出

为了方便后续处理和集成，**结构化输出**已成为提示词工程中常用的要求。也就是说，在Prompt中明确规定回答格式，如要求返回JSON、XML、Markdown表格、清单列表等。这样可以让模型生成**机器可读**、**易解析**的结果，减少人工整理步骤。

**实践技巧：**在提示中详细说明期望的结构。例如：“请以JSON格式提供结果，包含字段`name`（字符串）和`score`（数字）。” 模型若遵循，会直接给出形如：`{"name": "example", "score": 42}` 的输出，方便你的程序直接解析利用。又或：“请用Markdown表格列出比较结果，包含列A、B、C”。模型就会绘制表格。你也可以提供范例格式让模型套用（Few-shot 结合此技巧），例如给一个示例JSON输出，然后要求它对新输入也输出类似JSON。

**优点：**首先，结构化数据**减少了后处理开销**——比起从一大段散文中正则提取数据，JSON或CSV格式可以直接被程序读取。其次，它**提高一致性**——模型在严格格式约束下不太会天马行空跑题，只要格式对不上就说明需要重新生成或调整提示。

**适用场景：**在**代码生成**和**数据处理**场景下非常常见。例如生成API响应，要直接就是JSON字符串；让模型整理分析结果，用表格方式呈现关键信息。在需要将模型结果输入下游系统时（如把聊天结果存数据库），结构化输出能省却转换步骤。对于多条记录输出的情况，也可以让模型按特定分隔符列出，便于拆分。

**可能过时：**过去一些旧模型（GPT-3）对严格格式要求常常出错，需要反复试验提示。而2025年的模型已经更善于遵守格式指令，所以大胆要求结构化输出是新的常态。不过也要防范一点：模型有时**看似**给出了JSON但混入了评论或多余说明，破坏了纯格式。这种情况下可以在提示中强调“仅输出JSON，不要多余文字”。总的来说，**明确格式**这一做法在各模型间的适用性很强，不会因为模型升级而失效，反而随着模型改进变得更准确。

### 提示词耐久性与泛化

模型在不断升级，新版本对同一提示的响应可能发生变化。因此，提示词设计需考虑**耐久性**，避免依赖某一版本的特殊行为。重点在于使提示**通用泛化**，即换了模型或过了一段时间依然有效。

**策略1：避免利用模型漏洞或非官方行为。**例如以前有人利用ChatGPT的系统信息漏洞来获取答案，但OpenAI随时可能修复这种漏洞。如果你的提示建立在这类不稳定技巧上，很快就会失效。耐久的提示应当依赖模型**公开支持的功能**，如明确说明要求、提供范例、使用正确的角色指令等，而非某些内部命令或模型Bug。

**策略2：参数化和模块化**。将提示中的易变要素抽象为参数。例如不要把具体日期死写在提示里，而用占位符说明，让程序传入当前日期。这样当需求变化（比如阈值从100变200）时，你不用重写整个提示，只需调整参数。通过制作提示模板，并对**姓名**、**数量**等用`<NAME>`、`<NUM>`表示，你的提示在不同场景都能快速套用，不受细节变化影响。

**策略3：持续测试和迭代。**每当更换模型或模型版本更新，及时用你的提示跑一遍看结果是否一致。GPT-4.1对比GPT-4.0也许会在格式、严格程度上有差异。提前发现这些差异，你可以小幅调整提示适配新模型。如果有团队协作，也请**记录提示的版本和效果**，方便跟踪哪种写法在什么模型上表现最佳。

**适用场景：****生产环境**下的提示往往要长期使用且可能切换模型（例如从GPT-4换Claude或者接入新的Google Gemini）。这时提示的耐久性尤为重要。团队可以制定通用的提示模板标准，在不同模型上测试，通过的才纳入使用。对于**经常复用**的提示（如代码解释器、聊天机器人回复模板），更要保证其经得起模型更新。

**可能过时：**早期的提示工程有点像“黑客行为”，人们热衷发现特定模型的偏好和漏洞，用玄学手段骗出好结果。这些技能往往昙花一现，比如某版本喜欢某个口头禅，但下版就改了。因此2025年的共识是抛弃那些不透明的小技巧，转向**模型无关**的清晰沟通方式。只要你的提示遵循通用原则（清晰、具体、结构良好），它就更有可能在未来版本中保持有效。即使模型变聪明，不再需要某些啰嗦细节，一个健壮的提示也不会因为少了流行语就完全失效。

### 工具调用与 ReAct 框架的新实践

2024年以来，一个重要的新趋势是在提示工程中引入**工具调用**和**ReAct框架**。传统上，我们给模型一个大段提示，希望它凭内部知识直接回答所有问题；而**ReAct (Reason+Act)** 方法让模型可以**边思考边行动**：它可以查资料、调用计算工具、再依据结果继续推理，循环往复。这种思想与其说是一个提示，不如说是一种**提示链+工具使用**的工作流，被广泛应用于复杂任务的AI代理(agent)中。

**为什么ReAct更稳定？** 因为对于**知识密集**或**步骤繁多**的问题，单条提示往往不可靠：模型可能凭过时或不完整的训练知识编造答案。而采用ReAct，它可以在需要时**发出动作**（例如搜索互联网、调用计算函数），获取最新信息或精确计算结果，再将所得反馈纳入后续推理。模型不再闭门造车，而是有能力**查询和验证**。实践证明，在决策规划等复杂任务上，具备工具使用能力的ReAct智能体比单次Prompt表现更好，错误率更低。

当前最常用的工具包括：搜索引擎、数据库查询、计算器、代码执行器等等。许多框架（LangChain、LlamaIndex 等）可以将LLM与外部工具连接，实现ReAct式的问答。**例如：**用户问“今年北京的平均房价是多少，折合美元是多少？” 传统单Prompt模型可能不知道最新房价或胡乱估计美元汇率。但ReAct代理会先搜索最新房价数据，再调用换算工具，然后给出有据可依的回答。这种多步骤推理+行动的流程，由模型自己在提示引导下完成，比人工将所有资料糅进一个提示要可靠得多。

值得注意的是，ReAct本质上还是由一系列Prompt驱动的，只不过Prompt内容随着每次工具反馈不断更新。设计ReAct风格Prompt时，要告诉模型**何时该思考，何时该行动**。通常模板如：“思考：…\n行动：<工具指令>\n观察：<工具返回>\n… 最终回答：…” 等等，模型据此循环。这种明确的格式引导模型交替输出思考和动作，大大提高了解题的**可解释性**和**成功率**。

**适用场景：**当任务**超出模型训练知识**（需要实时信息）、或**需要多步决策**（搜集信息->分析->决策）时，ReAct框架最有优势。例如**信息检索问答**、**数据分析**、**复杂规划**（旅行计划、故障诊断）等，都可以通过让模型调用检索/计算工具来完成。2025年的大型模型（如Google Gemini）甚至在架构中原生支持工具使用和多模态输入，使ReAct思想更易实现。在编程助手场景，模型可以不断尝试运行生成的代码，根据错误信息修正代码，这也是一种ReAct形式的应用。

**可能过时：**相比之下，“单提示大而全”已经逐渐不被推荐用于复杂任务。如果你还在用一条Prompt问GPT一切，希望它凭记忆回答所有细节，在2025年这是不稳定且有限的。取而代之的是Prompt Engineering与**流程编排**的结合：协调模型逐步完成任务。可以认为，提示词工程正从“写一个完美的提示”进化为“设计一套完善的提示交互流程”。这是AI应用成熟的标志。今后随着工具接口标准化，这一趋势会更明显。因此不要拘泥于老式的一步到位提示——尝试让模型以ReAct方式工作，你会发现它在解决复杂问题上游刃有余。

## 自查清单

在编写完提示词后，使用以下清单自检，确保提示涵盖了高质量提示的要素：

- **[ ] 任务目标是否明确：** 我的提示是否清楚描述了我要模型完成的具体任务和期望？有无歧义或过于宽泛的表述？
- **[ ] 约束条件是否补充：** 我是否提供了必要的背景信息、输入数据或特定要求（长度、风格、性能指标等）供模型参考？是否明确了不能触碰的禁区（如果有）？
- **[ ] 输出格式是否指定：** 我有没有明确要求模型以特定格式输出结果（如JSON、表格、代码块），以方便阅读或后续处理？
- **[ ] 语言表述是否清晰简洁：** 提示中用了直接的动词指令，避免了冗长的废话或模棱两可的词语？主要问题是否放在了提示的最后以突出重点？
- **[ ] 是否需要拆分任务：** 这个提示是否在让模型一次完成太多事情？如果是，是否可以改为多轮对话，逐步引导模型完成每一步？
- **[ ] 是否提供示例：** 对于格式或风格要求高的任务，我有无提供一两个范例让模型参考？这些示例是高质量且与任务强相关的吗？
- **[ ] 角色定位是否得当：** 我是否需要（或已经）在提示中赋予模型特定角色，以利用该角色的视角和知识？这个角色对任务是否有帮助，而非徒增限制？
- **[ ] 避免过时技巧：** 我的提示中是否避免使用了陈旧、低效的套路？比如没有无谓地在简单任务上加入“逐步思考”的口头禅，也没有依赖某个模型独有的“咒语”或漏洞。
- **[ ] 考虑不同模型：** 如果要在不同模型上使用（GPT-4、Claude、Gemini等），我的提示在措辞上是否通用？我是否针对目标模型的特点做了相应调整（如长度适配Claude的大上下文）？有没有在新模型上测试确认输出正常？
- **[ ] 结果校验与迭代：** 凭该提示得到模型初步输出后，我计划如何评估和改进？是否设计了追问让模型自我检查关键细节？对于重要任务，是否准备了一些对抗性提问来验证模型不会胡说？

通过以上自查，您将能**完善地优化自己的提示词**，避免常见陷阱并紧跟最新的提示工程实践。记住：Prompt Engineering 不是一成不变的套路，而是一套**动态演进的沟通艺术**。持续关注新方法并结合这些清单反思，才能让你的提示始终走在时代前沿，驱动模型产出令人满意的结果！
