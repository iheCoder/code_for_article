## 上下文工程：从“炼金术”到“方法论”，构建高确定性AI系统的核心引擎



在大型语言模型（LLM）的实践中，我们常常陷入一种矛盾的狂热：一方面惊叹于它涌现出的强大能力，另一方面又对其输出的“不确定性”感到沮 fous。我们像中世纪的炼金术士，反复调试提示词（Prompt），试图找到那组能点石成金的“咒语”。

然而，真正构建企业级、高可靠性的AI应用，靠的绝不是“炼金术”，而是一套严谨的“方法论”。**上下文工程（Context Engineering）**正是这套方法论的核心。它将我们的角色从“提示词魔术师”转变为“信息架构师”，其核心目标只有一个：**通过系统化地设计、输送和管理信息，最大限度地降低LLM推理的“认知负荷”，从而将不确定性转化为高确定性的输出。**

这篇文章将深入探讨上下文工程的本质，并为你铺开一条从理论到实践的清晰落地路径。

## 第 1 章｜定义与边界：上下文工程究竟是什么、不是什么

### 1.1 定义

**上下文工程（Context Engineering）**：

在给定任务与预算下，**选择、组织并按时送达**对模型最关键的**信息与工具**，让它**可核验、可执行、稳定**地完成工作。

> 三个动词是核心：**选择**（检索/筛掉噪声）、**组织**（压缩/排版/设定输出契约）、**按时送达**（在合适的步骤喂入，并在需要时调用工具）。

> 其它扩展维度（权限、评测、成本等）是落地时的配套要求，放到 1.3 说明——定义里不一次性塞满。



### 1.2 为什么需要这门“工程”

1. 模型并不“过目不忘”。长上下文会出现**位置偏置**：对开头、结尾更敏感，中间最容易被忽略。把 100 页原文全塞进去，只会让关键信息**被噪声埋没**。

2. 企业问题强调**可核验与合规**：答案需要能指向证据，还要遵守权限边界。

3. **成本与延迟**是硬约束：token、Rerank、调用链都要钱、要时间。

   上下文工程解决的正是这三件事：**控噪声、保证据、守预算**。

### 1.3 与其它路线的边界（知道“不是什么”更重要）

- **RAG（检索增强生成）**：上下文工程的“骨干肌肉”。当你需要**最新/专有知识 + 可核验引用 + 权限**时，它是默认选项。
- **长上下文（Long Context）**：适合**单文档通读/风格保持**的场景，但别把它当“万能桶”；仍需**压缩与布局**避免中段丢失。
- **微调（Fine-tuning）**：当任务**格式稳定/风格固定/术语统一**，用小中模型微调可提稳健性与单位成本；但**知识频繁更新或需要证据**时，优先上下文工程，再视需要叠加微调。

### 1.4 三条设计铁律

#### 最小必要上下文（Least Necessary Context）

- **做法**：只放“对这次决策有直接贡献”的片段；其余一律留在检索层。

- **为什么**：上下文是“显存”，不是仓库。多一个无关段落，就多一份注意力被分走的可能。

- **例子**：问“**退货邮费谁承担？**”



- ✅ 放：退货条款里“**退货邮费由商家承担**”的句子 + 生效日期。
- ❌ 不放：运费险定义、仓库地址、与退货无关的“换货流程”。



#### 证据可核验（Citations First-Class）

- **做法**：答案必须能回指到**原文片段**或**外部工具的可追溯结果**；在生成前后做**引用对齐检查**。
- **为什么**：这是“可信”的唯一硬边界，能防止“看似对但无出处”的幻觉。
- **例子**：回答“退款时效 7 天”，旁边标注【来源：售后政策 §3.2，第 2 句】；若证据缺失则**拒答**或提示需要人工/外部系统确认。



#### 显式契约：结构化输出 + 工具调用

- **做法**：用 **JSON Schema** 限定输出；需要行动时走**函数/工具调用**（查库、跑 SQL、生成报表），模型只给参数。

- **为什么**：自由文本不可控、难重试；契约化让**自动化**成为可能。

- **例子**：

    - Schema：{"policy":"string","deadline":"date","is_refundable":"boolean","citations":[...]}
    - 如果要查“用户是否在可退期”，让模型调用 check_return_window(order_id)，而不是让它**猜**。



### 1.5 关键构件小词典

- **切块（Chunking）**：把文档按**语义/结构**切成小段，便于检索与引用。表格、代码要用**专门切块器**。

- **召回（Recall）**：从海量语料挑出**可能相关**的片段，拿到一个候选集合（Top-K）。

- **重排（Rerank）**：对候选集合逐段“深入比对”，把**看起来像**但**其实不相关**的剔除。

- **硬负例（Hard Negative）**：**表面相似但内容无关**的迷惑片段，会强烈干扰模型判断。

    - **生动例子**：
        - 问题：*“退货邮费谁承担？”*
        - 片段 A（正例）：*“非质量问题退货，****退货运费由消费者承担\****。”*
        - 片段 B（**硬负例**）：*“****运费险\****由保险公司承担，理赔需在 48 小时内申请。”*
        - 它们都出现“运费/承担”，但 B 讨论的是**保险赔付**，不是**退货运费**。把 B 也塞进上下文，模型极易得出“商家或保险公司承担”的错误结论。



- **HyDE**：让模型先写一段**“假答案的短文”**，拿它去向量检索，提高“问题太短/没标注”场景下的召回质量。

- **位置偏置（Lost-in-the-Middle）**：长上下文里，中间的信息最容易“糊掉”；把关键信息前置是常用对策。

- **结构化输出（Structured Outputs）**：用 Schema 让答案“对齐可解析格式”。

- **函数/工具调用（Function Calling / Tools）**：把“要做事”的部分交给外部系统执行；模型只负责“**决定做什么**”与“**填参数**”。

- **GraphRAG**：把文本抽成**实体-关系图**，先做“社区级摘要”，再结合原文检索，适合**多实体/多跳**问题。

- **Prompt Caching**：平台对**相同前缀内容**（系统指令/Schema/工具说明/示例）做缓存，重复使用更快更便宜——所以要把**稳定内容**放在提示的**最前**。

### 1.6 常见反模式

#### 反模式 A “把 Top-K 一味拉大”

- **现象**：从 10 拉到 50、100，觉得“多多益善”。

- **为什么坏**：K 变大，**硬负例**的比例陡增；模型注意力被分走，答案变得**似是而非**。

- **怎么改**：



1. 先**多路召回**（向量 + 关键词 + 多查询/HyDE），得到 30–50 个**候选**；
2. 用 **Rerank** 把候选压到 **Top-5/Top-8**；
3. 给每段证据打**一致性分**，低于阈值就不带进上下文。



- **例子**：问“退货邮费谁承担？”



- 拉 50 段：出现“运费险条款”“发货运费”“邮寄地址”等**硬负例**；
- 用 Rerank 压到 5 段：只剩“退货条款 + 生效时间 + 例外情况”，答案稳定。



#### 反模式 B｜“只用向量检索”

- **现象**：把 BM25（关键词）全关掉。
- **为什么坏**：**编号、术语、公式、代码符号**对语义向量并不敏感，容易漏召回。
- **怎么改**：**混合检索**（向量 + BM25）+ **融合排序**（如 RRF），保证“语义与字面”都照顾到。
- **例子**：问“**表 A 第 12 条**怎么规定？”只靠向量，很可能错过“第 12 条”的精确匹配。



#### 反模式 C｜“能塞就塞，整本手册进上下文”

- **现象**：害怕漏信息，把整篇放进去。

- **为什么坏**：触发**位置偏置**，中间信息易丢；同时**成本陡增**。

- **怎么改**：



- **预算化压缩**：先抽关键句；
- **布局优化**：把关键信息放在前部；
- **引用对齐**：保留原文片段 ID，答案里标注来源。



- **例子**：把“退货流程图”整页贴进去不如摘出“承运人遴选不影响邮费承担方”的一句关键话。



#### 反模式 D｜“自由文本输出，一把梭”

- **现象**：让模型写“自然语言长段落”，下游系统读不懂。

- **为什么坏**：不可解析、不可重试、不可审计。

- **怎么改**：**JSON Schema** + **函数调用**。

- **例子**：



- ❌ “您可以退货，邮费我司承担。”
- ✅ {"is_refundable":true,"postage_payer":"merchant","policy":"售后政策§3.2","citations":[{"doc":"policy_v7","sec":"3.2","sent":2}]}



#### 反模式 E｜“权限后补”

- **现象**：先检索、后过滤。
- **为什么坏**：即便最后没展示，**越权检索**也可能在日志里留下痕迹。
- **怎么改**：**检索前**按租户/角色做**索引级过滤**；**检索后**再做结果级过滤，双保险。



#### 反模式 F｜“不设预算，不看延迟”

- **现象**：为了“更准”，一路加模型、加上下文、加外部工具。
- **为什么坏**：等你“准”的时候，用户已经走了。
- **怎么改**：在 PRD 里就定下**P95 延迟/单位成本**，通过**压缩、缓存、模型级联**守住。



### 1.7 一个“带数字”的迷你案例（从检索到答案，走一遍）

**问题**：*“7 天无理由退货，邮费谁承担？”*（期望 2 秒内答复，必须给出处）



**Step 1｜召回（30–50 条）**

- 向量检索：召回 30 条；
- 关键词（BM25）：“退货”“邮费”“承担”精确匹配：召回 25 条；
- HyDE 生成人类可读“假答案”，再向量检索：新增 10 条；
- 合并去重得到 **50 条候选**，先按**权限**过滤掉跨租户内容。



**Step 2｜重排（Top-50 → Top-6）**

- 用 cross-encoder/Rerank 逐段比对“问句 ↔ 片段”；
- 设**一致性阈值 0.6**，低于阈值剔除；
- 结果保留 **6 段**：包括“退货条款 §3.2”、“例外情况（大件/生鲜）”、“生效时间”。



**Step 3｜压缩与布局（塞进 800 tokens 预算）**

- LongLLMLingua 提取每段**关键句**并去重；
- 把“**承担方**与**例外**”放在上下文**最前**；
- 在每段后保留 {doc_id, section, sentence_id} 作为**引用锚点**。



**Step 4｜编排与执行**

- System 提示写清：要**JSON 输出** + **必须引用**；
- 如果用户附了订单号，触发 check_return_window(order_id) 工具，确认是否仍在 7 天内；
- 模型合成答案，填入 postage_payer="consumer|merchant|platform|unknown"，并附引用列表。



**Step 5｜答案（机器可读 + 人能懂）**

```
{
  "is_refundable": true,
  "postage_payer": "consumer",
  "policy": "售后政策 v7 §3.2",
  "exceptions": ["质量问题由商家承担", "生鲜不适用7日"],
  "citations": [
    {"doc":"policy_v7","sec":"3.2","sent":2},
    {"doc":"policy_v7","sec":"3.5","sent":1}
  ]
}
```

> 展示给用户的自然语言可由这个 JSON 二次渲染；一旦政策更新，只需重建索引即可，无需改模型。



## 1.8 小结（把这一章落成“评审清单”）

- **定义**：选择 / 组织 / 按时送达 **关键信息与工具**——为**可核验、可执行、可控**服务。

- **铁律**：最小必要上下文｜证据可核验｜结构化输出 + 工具调用。

- **反模式**：Top-K 盲目增大｜只用向量不混检｜整本塞上下文｜自由文本无契约｜权限后补｜不设预算。

- **会做的三件事**：



1. 先把**证据**做对（混合召回 + 重排 + 压缩 + 布局）；
2. 再把**输出**做稳（Schema + 函数调用 + 引用对齐）；
3. 最后把**成本/延迟/权限**做实（预算 + 缓存 + 双层 ACL）。



### 结语



从“大海捞针”式的提示词调试，到构建一个精密的、自动化的信息处理流水线——这就是上下文工程带来的认知飞跃。它虽然复杂，但它将AI应用开发从一门“艺术”变成了一门真正的“工程科学”。

放弃寻找“完美提示词”的幻想吧。真正的护城河，在于你如何为你的大模型构建一个无与伦比的、高效的信息供给系统。这，就是上下文工程的全部意义所在，也是通往通用、可靠人工智能的必由之路。