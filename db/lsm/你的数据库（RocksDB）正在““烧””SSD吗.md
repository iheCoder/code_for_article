## 你的数据库（RocksDB）正在““烧””SSD吗？—— 深入写放大、LSM-Tree与RefinedMOR的““全局最优””博弈



嘿，各位开发者！

我们每天都在和数据库打交道。你是否想过，当你开开心心向RocksDB（或者TiDB, CockroachDB, YugabyteDB等）写入1MB数据时，你的SSD可能在““尖叫””——它实际写入了30MB、50MB甚至100MB的数据。

这不是危言耸听。这个现象叫做**写放大（Write Amplification, WA）**。



它就像一个潜伏在你系统里的““性能刺客””，悄悄地：

1. **消耗你宝贵的I/O带宽**：让你的写入操作变慢。
2. **““燃烧””你SSD的寿命**：SSD的P/E（编程/擦除）周期是有限的。

LSM-Tree（Log-structured Merge-tree）14141414是这一切的““万恶之源””，也是““性能之源””。它通过““牺牲读、换写入””的设计哲学，提供了超高的 ingest 吞吐量。但它也带来了写放大的““原罪””。

近期，一篇来自波士顿大学、Meta等机构的硬核论文 **《Benchmarking, Analyzing, and Optimizing Write Amplification of Partial Compaction in RocksDB》**，对RocksDB的心脏——**局部压缩（Partial Compaction）**——进行了一次““开胸手术””。

这篇论文的结论非常““颠覆””，它用翔实的实验**推翻了**许多我们（包括官方文档）过去的““调优圣经””18181818。更重要的是，它揭示了一个深刻的系统设计问题：**““贪心””的局部最优，往往是““全局””灾难的开始。**

今天，我们就以这篇论文为““手术刀””，深入LSM-Tree的““脏腑””，看看我们到底该如何驯服““写放大””这个猛兽。



### 1. 为什么是LSM-Tree？““快””的代价

想象一下，如果你用B+Tree（比如InnoDB）来处理海量写入，每次`INSERT`或`UPDATE`都是一次随机I/O。当数据量一大，B+Tree的层级变深，为了维护树的平衡，你可能需要不断地分裂和移动页（Page），这在机械硬盘时代是灾难，在SSD时代也代价高昂。

LSM-Tree说：我““懒””得管了。

1. **写入（MemTable）：** 你来多少数据，我全都在内存里的**MemTable**（一个有序数据结构）里““笑纳””。

2. **刷盘（Flush）：** MemTable写满了，我也不““合并””旧数据，而是直接把它变成一个**不可变（Immutable）\**的MemTable，然后““pia””地一下，完整地、有序地、一次性地刷（Flush）到磁盘上，变成一个\**SST（Sorted String Table）文件**。这个文件落在**Level 0 (L0)**。

3. **L0的““混乱””：** L0是LSM-Tree里最““脏乱差””的地方。因为SST文件是一个接一个刷下来的，它们彼此之间的键（Key）范围是**重叠**的。



这个设计，让LSM-Tree的**写入（Ingest）**变成了纯粹的““顺序写””，快到飞起。

但是，““懒””是要还的。

- **读放大（Read Amplification, RA）：** 因为L0的文件是重叠的，你查一个Key，最坏情况下，你得把L0**所有**的SST文件都查一遍。这导致了极高的**读放大**。
- **空间放大（Space Amplification, SA）：** 你`UPDATE`了一个Key 100次，磁盘上（L0里）就有这个Key的100个““尸体””（旧版本）。这导致了极高的**空间放大**。



### 2. Compaction（压缩）：““还债””的开始

为了解决RA和SA，LSM-Tree必须““还债””。这个““还债””的过程，就叫**Compaction（压缩）**。

Compaction会定期启动，干两件事：

1. 把L0的““混乱””文件，和L1的““有序””文件合并（归并排序）。
2. 在合并过程中，把““旧版本””的Key和““已删除””（Tombstone）的Key扔掉。
3. 生成**新的、更大、更少**的SST文件，写入L1。

L1满了，就和L2合并，以此类推。从L1到LN（最深层），SST文件都是**有序且不重叠**的，查询很快。

**““写放大””（WA）就在这个““还债””过程中诞生了** 。



> **写放大 (WA) = 真正写入存储的字节数 / 用户请求写入的字节数**

你写入1MB数据到L0。为了把它““推””到L1，它和L1的9MB数据合并了，**写入了10MB的新文件**。然后这个10MB为了““推””到L2，和L2的90MB数据合并了，**又写入了100MB的新文件**。

看到了吗？你只写了1MB，磁盘却““吭哧吭哧””写了（1 + 10 + 100）= 111MB的数据。WA高达111倍！



### 3. 核心难题：Full Compaction vs. Partial Compaction

早期的LSM-Tree（比如LevelDB）使用**Full Compaction（全量压缩）**。

- **做法：** L(i)层满了，L(i)层**所有**文件，和L(i+1)层**所有**重叠文件，全部拉出来，一起““大锅烩””。
- **问题：** 灾难性的**延迟毛刺（Latency Spikes）**。想象一下L3（几百GB）和L4（几TB）做全量压缩，你的I/O会瞬间爆炸，系统““假死””。

RocksDB学聪明了，它采用了**Partial Compaction（局部压缩）**。

- **做法：** 为了实现““局部””，RocksDB把L1-LN的每一层都切成很多个**固定大小的SST文件**（比如64MB）。
- **关键：** 当L(i)层需要压缩时，我**只挑选L(i)中的一个（或几个）文件**，去和L(i+1)中跟它重叠的文件合并。

这样就把一次大的I/O风暴，摊销（amortized）成了很多次小的I/O““微风””。

好了，论文的核心问题来了：

> **如果L1有100个SST文件，我到底该““挑””哪一个文件去压缩呢？**

这个决策，就是**File-Picking Policy（文件挑选策略）**。你的选择，将**极大**影响系统未来的WA。



### 4. ““四大门派””：RocksDB文件挑选策略的利弊

这篇论文重点分析了RocksDB的四种策略。让我们来深度剖析它们的““武功””和““罩门””。



#### (a) MinOverlappingRatio (MOR)：““贪心””的““最优””



- **工作原理：** 这是最““符合直觉””的““贪心””策略。它会计算L(i)中**每个文件**与L(i+1)的““重叠率””。



`重叠率 = L(i+1)中重叠文件的总Bytes / L(i)这个文件的Bytes`





- **决策：** 挑选那个**重叠率最小**的文件 424242。





- **利 (Pros):**

    -

    **局部最优：** 显而易见，这个选择使得**““本次””压缩**的WA最小 43。我只读/写最少的数据。





-

**大规模““基石””：** 论文的一个关键发现是，在**大规模（如40GB+）\**且\**更新均匀（Uniform）\**的负载下，MOR的\**平均WA是最低的，且稳定性（抖动）也是最好的** 44444444。它是一个非常优秀的““默认””和““兜底””选项。





- **弊 (Cons):**

    - **““贪心””的诅咒（不稳定）：** MOR只看眼前。论文发现，MOR最大的问题是**不稳定（unstable WA）**。在相同特征的工作负载下，跑10次，WA可能相差巨大。



-

**短视的““破坏者””：** 这是MOR的““原罪””。它““贪心””地选择了**当前**最优的`File A`（重叠率1.0），却可能**破坏了**一个**未来**的““黄金压缩””机会（比如`File B`和`File C`，它们各自的重叠率是1.1，但如果它们不被`File A`的压缩所““污染””，它们未来可以独立地被压缩，总WA更低）。







#### (b) RoundRobin (RR)：““轮盘””的““公平””



-

**工作原理：** ““雨露均沾””策略。它在L(i)层维护一个““游标””（Cursor），指向上次压缩的**最大键（Key）**48。





-

**决策：** 下次压缩时，它会从游标开始，选择**下一个**键范围的文件 49。





-

**注意：** 论文强调，RocksDB的**新版RR** 505050已经比LevelDB的老版RR智能得多。它会配合文件分裂（Splitting）机制，确保**整个键空间（key space）**被严格均匀地压缩，而不是简单地““下一个文件””51。





- **利 (Pros):**

    -

    **““热点””克星（论文颠覆性发现）：** 这是论文的**““杀手级””发现**。在更新倾斜（Skewed，即热点）负载下，**RR的WA显著优于MOR** 52525252。这推翻了““热点用OLSF””的旧建议 535353。





-

**为什么？** RR的““迟钝””是故意的。它““轮””到热点文件才压缩。这给了这个热点文件**更长的时间停留在L(i)层**5454。在这段时间里，L0刷下来的**新**版本数据，可以在L(i)（或L0->L1）的压缩中直接““覆盖””它，而**无需**把它““推””到L(i+1)再覆盖 55。这大大减少了““热点””数据被无效““搬运””的次数。





- **弊 (Cons):**

    -

    **““机械””的公平：** RR不““看脸””。如果““轮””到的文件恰好和L(i+1)有巨大重叠，RR也会硬着头皮上，导致**单次**压缩的WA很高，I/O抖动大 56。





- **““均匀””负载下的平庸：** 在更新均匀的负载下，它的表现不如MOR。



#### (c) OldestLargestSeqFirst (OLSF)：““挑最冷””的““考古””



-

**工作原理：** 挑选L-seq（Largest Sequence number）**最小**的文件 57。





- **L-seq是什么？** 是SST文件中““最新””的那个Key的序列号。L-seq最小，意味着这个文件里连““最新””的数据都““最旧””了。

-

**决策：** 挑选最““冷””的那个文件 58。





- **利 (Pros):**

    - （理论上）适合““冷热分离””的场景，把““冷””数据赶紧““发配””到更深的Level。

- **弊 (Cons):**

    -

    **““理论””破产：** 论文实验**彻底否定了**““热点负载用OLSF””的旧建议 59。在所有热点测试中，它都被RR和MOR**吊打**60606060。







#### (d) OldestSmallestSeqFirst (OSSF)：““挑最老””的““历史””



-

**工作原理：** 挑选S-seq（Smallest Sequence number）**最小**的文件 61。





- **S-seq是什么？** 是SST文件中““最老””的那个Key的序列号。

-

**决策：** 挑选**真正**““最古老””的那个文件 62。





- **利 (Pros):**

    - （理论上）对数据时效性（TTL）场景友好，把最老的文件尽快““推””到最后一层，以便于删除过期数据。

- **弊 (Cons):**

    -

    **““理论””再次破产：** 论文也**否定了**““随机更新用OSSF””的旧建议 63。在随机更新负载下，它的WA表现同样不如MOR和RR 64646464。





------



### 5. ““贪心””的诅咒：为什么MOR会导致““全局””灾难？



论文的第4节 65 是全篇的华彩，它深入分析了MOR的**不稳定性（Instability）**。





核心洞察 6666： MOR只看““当下””的WA，但它““贪心””的选择，会**““污染””**（destroy）其他文件**““未来””**的低WA压缩机会。



让我们用论文中图13 67 的例子来复现这个““悲剧””：



1. 初始状态 (State a) 68



- **L1层**有4个文件，键范围分别是 [1-10], [11-20], [21-30], [31-40]。

- **L2层**也有对应的4个文件 [1-10], ..., [31-40] （假设它们之前是被““Trivial move””下来的）。

-

**此时，L1的4个文件，重叠率都是 1.0** 69696969。





- **MOR的““困境””：** 4个都一样好，选哪个？在真实负载下，这4个文件的重叠率可能是 1.0, 1.01, 0.99, 1.02。MOR会““贪心””地选择那个0.99的。我们就假设它选了**第一个文件 [1-10]**。

- ““贪心””的选择：压缩 L1 [1-10] (Path 1) 70707070



- L1 [1-10] 和 L2 [1-10] 合并，在L2生成新文件。

- L1层现在““空””出了 [1-10] 的““洞”” 71。L1剩下：[11-20], [21-30], [31-40]。





-

**灾难的““序幕””：** 此时，L0刷下来一个大文件，键范围是 [1-55] 72。





- **““污染””发生：** 这个L0文件在L1层被切割。

    - [1-10] 部分：落在 L1的““洞””里，生成L1新文件 [1-10]（或[1-xx]）73。





- [11-20] 部分：它本可以独立，但为了对齐，它**被迫**和L1现有的 [11-20] 文件**合并**（或者说，L0的[1-55]文件在L1被切割时，会““参考””L2的边界 74747474，导致[1-10]和[11-20]的数据被““黏合””在一起，生成一个[1-20]的大文件）75。





- **结果：** 原本““干净””的、重叠率为1.0的 L1 [11-20] 文件，**被““污染””了**。它**失去了**未来独立进行1.0重叠率压缩的机会。

- ““最优””的选择：压缩 L1 [31-40] (Path 2, Optimal) 76767676767676



- ““最优””策略（通过暴力搜索得知 77）会选择压缩**最后一个文件 [31-40]** 78。





- L1层““空””出了 [31-40] 的““洞””。L1剩下：[1-10], [11-20], [21-30]。

- **L0的 [1-55] 文件刷下：**

    - [1-10] 部分：落在L1，生成 [1-10] 文件。
    - [11-20] 部分：落在L1，生成 [11-20] 文件。
    - [21-30] 部分：落在L1，生成 [21-30] 文件。
    - [31-55] 部分：落在L1，生成 [31-xx] 和 [xx-55] 文件。

-

**结果：** L1的 [1-10], [11-20], [21-30] 三个文件的““黄金压缩””机会**被““保留””（preserved）了下来**79。





-

结论 80808080： MOR的““贪心””（Path 1）**破坏了**未来的““全局最优””（Path 2）。它为了**眼前 1%** 的利益，导致了**未来 50%** 的损失。





------



### 6. RefinedMOR的““全局””智慧（Algorithm 1 深度解析）



基于““贪心””的诅咒，论文作者们提出了一个改进算法：**RefinedMOR** 81818181。



**RefinedMOR的核心思想：**

> **““在‘“差不多好’”的选项里，挑一个对‘“未来’”最有利的””**。

它通过一个两步决策过程实现：

步骤 1：筛选““候选集””（Candidate Set）8282



RefinedMOR 不会只选““最小””的那一个。它引入了一个““门槛”” `th`（例如 `th = 0.05`，即 5%）83。



1.

**[Line 1]** `f = MOR(files)`：首先，还是用MOR找到真正的““最小重叠率”” `f.r` 84。





2.

**[Line 3]** `if files[j].r < f.r * (1 + th)`：遍历L1所有文件`j`。如果`files[j]`的重叠率 `files[j].r` 并不比““最小””的 `f.r` 差太多（比如在5%以内），那么它就是““好””的，**““入围””**，成为““候选集””85。





**类比：** 招聘。我不是非要那个99分的（`f.r`），所有95分（`f.r * 1.05`）以上的，我都要进““面试””（候选集）。

步骤 2：在““候选集””中““精挑””（Re-rank）868686



这是最精髓的地方。我们有了 3 个““95分””以上的候选者，选谁？

**RefinedMOR的““二阶启发””是：**

> **““优先‘“牺牲’”那个‘“邻居’”最差的！以‘“保护’”那些‘“邻居’”好的‘“黄金组合’”’。””**

这个逻辑在算法里是这样实现的：

1. **[Line 4-5]** `if j == files.size() - 1 then return files[j]`：

    -

    **特殊处理：** 如果““候选者”” `j` 已经是L1的**最后一个文件** 87。





-

**决策：** 马上选它！88





- **为什么？** 它没有““邻居””（`j+1`文件）了。压缩它，不会““污染””任何`j+1`。它是**最““安全””**的选择。

2. **[Line 6]** `files[j].r_th = th * files[j].r - files[j + 1].r`：

    -

    **核心中的核心！** 89





- `files[j].r_th`：这是为““候选者”” `j` 计算的一个**““新分””**（`r_th`代表Refined-th）。

- `th * files[j].r`：这是`j`的““原始分””（乘以`th`是为了加权，先不管）。

- `- files[j + 1].r`：**减去！** 减去它**““邻居””**（`j+1`文件）的重叠率！

- **这意味着什么？**

    - 如果`j`的““邻居””`j+1`**很““差””**（重叠率`files[j+1].r`**很高**），那么`j`的新分`r_th`就会**很““低””**。
    - 如果`j`的““邻居””`j+1`**很““好””**（重叠率`files[j+1].r`**很低**），那么`j`的新分`r_th`就会**很““高””**。

3. **[Line 7]** `return MOR(files)`：

    -

    **最后一步：** 在所有““候选者””的““新分””`r_th`上，**再跑一次MOR** 90。





- **结果：** 那个**`r_th`最低**的““候选者””`j`，会被选中。

- **翻译：** **那个““邻居””`j+1`最““差””（重叠率最高）的““候选者””`j`，会被选中！**

**RefinedMOR的““全局””智慧：**

- 它看到了MOR的““贪心””悲剧（Path 1）。

- 它的目标是**““保护””（Preserve）**那些““邻居””`j+1`也一样好的““黄金压缩””机会 91。





- 它的策略是““牺牲””那些““邻居””`j+1`本来就很““烂””的文件`j`。反正`j+1`已经很烂了，我压缩`j`，不会““污染””`j+1`（因为它已经““脏””了），也就**不会破坏任何““未来””**。

效果如何？

RefinedMOR实现了““降维打击””。它在保持了MOR低平均WA的同时，极大地提高了稳定性。在40GB负载下，它将WA的““抖动””（四分位距）降低了 75.1%92，并且平均WA也是所有策略中最低的93939393。



### 7. 我们的““藏宝图””：给程序员的10个启示



这篇论文 94 不仅仅是RocksDB的““调优指南””，它给我们的启示是多方面的：



1. 扔掉““旧圣经””，相信““新数据””95959595



- **旧建议：** ““随机更新用OSSF””。

-

**论文打脸：** OSSF在随机更新下，平均和最大WA都**更差** 96969696。





- **旧建议：** ““热点更新用OLSF””。

-

**论文打脸：** OLSF在热点（Skewed）更新下，WA**显著差于**RR和MOR 97979797。





-

**启示：** 技术的““常识””是会过期的。软件在进化（如RocksDB的新RR 98和新分裂机制 99999999），硬件在进化（如Optane vs SATA 100）。**永远用你当前版本、真实负载去Benchmark**。







#### 2. ““热点””负载的““新王””：RoundRobin (RR)



-

**新发现：** 在**更新倾斜（热点）**负载下，**RR的WA最低** 101101101101。





-

**启示：** 如果你的业务是““抢购””、““积分””、““计数器””，别用OLSF，**去试试RR**。RR的““延迟””压缩机制 102102102102，反而让热点数据在L1被““便宜””地覆盖了。







#### 3. ““稳定””的基石：MinOverlappingRatio (MOR)



-

**新发现：** 在**大规模、更新均匀**的负载下，**MOR的平均WA最低且最稳定** 103103103103。





- **启示：** MOR依然是““默认””下的最强王者。如果你的业务是““海量日志””、““物联网时序””（均匀写入），MOR是你的““定海神针””。



#### 4. ““白给””的优化：高比例更新（>60%）



-

**新发现：** 当更新比例非常高时（例如超过60%），**四种策略的WA都差不多，而且都很低** 104104104104。





-

**启示：** 别““瞎””调了。因为大量的““旧数据””在内存Buffer或者L0压缩时，就被新数据““干掉””了（去重）105105105105，根本没机会““下沉””到L1引发写放大。







#### 5. ““贪心””的诅咒：P99比Avg更重要



-

**新发现：** MOR的““平均””WA很低，但**““方差””**（不稳定性）很高 106106106106。





- **启示：** 作为系统架构师，你关心的不应该是““平均延迟””，而是**““P99/P999延迟””**。一个““平均””很快，但““偶尔””巨慢的系统（MOR的不稳定性）是生产环境的灾难。



#### 6. RefinedMOR的智慧：““保留未来””



-

**新发现：** RefinedMOR的““二阶启发””107核心是““保护””那些““好””的““邻居””108。





- **启示：** 这是最精髓的架构思想。**““不要过早优化””，也不要““贪心””地用掉你““当下””的““最优解””**。一个好的系统，永远在““决策””和““保留选项””之间做平衡。



#### 7. 硬件的““反直觉””



-

**新发现：** 用**慢速SATA SSD**，WA居然比Optane**更低**！109





-

**为什么？** 因为SATA盘太慢，导致L0的压缩**积压（Stall）**110。L0的文件堆积如山（比如从8个堆到10个）111。这反而““因祸得福””，让更多数据在L0就被合并去重了 112。





-

**启示：** 这绝对是个““陷阱””。你用WA换来的是**极高的空间占用和灾难性的查询延迟（RA）**113113113113。这再次证明了RUM猜想 114——WA、RA、SA，你不可能““全都要””。







#### 8. 文件大小（`fs`）不重要，层级比例（`T`）才重要



-

**新发现：** `target_file_size`（`fs`）从16MB调到128MB，对WA**几乎没影响** 115115115115。





-

**新发现：** `size_ratio`（`T`）从4调到10，WA**显著增加** 116。





-

**启示：** 别再““玄学””调`fs`了。`T`才是WA的““命门””，这符合`O(T*L/2)`的理论模型 117117117117。







#### 9. ““Trivial Move””永远是““白给””的最优解



-

**新发现：** 论文对比了““跳过””（`skip`，即优先Trivial Move）和““不跳过””（`non-skip`）两种暴力搜索策略，发现WA**几乎一样** 118。





-

**启示：** Trivial Move（当L(i)的文件和L(i+1)不重叠时，直接““瞬移””下去，0 WA）永远是最好的 119。RocksDB的这个优化是绝对正确的。







#### 10. ““全局最优””是可以““逼近””的



-

**新发现：** MOR的**最小WA**（即““运气好””的时候），其实已经**非常接近**““理论最优””了 120120120120。





- **启示：** 我们的目标，不是去发明一个““神””算法，而是像RefinedMOR一样，用一个**简单的、低成本的““启发式””**（Heuristic），去**““规避””**掉MOR““运气差””的**““最坏””情况**。

- **最终启示：** **一个““稳定””的90分，远胜一个在0分和100分之间““抽搐””的““天才””。**

------

**参考文献**

- [1] Wei, R., Zhuang, J., Zhu, Z., Kryczka, A., & Athanassoulis, M. (2025). *Benchmarking, Analyzing, and Optimizing Write Amplification of Partial Compaction in RocksDB*. In Proceedings of the 28th International Conference on Extending Database Technology (EDBT). (即本文分析的源论文)
- [2] Facebook RocksDB. https://rocksdb.org/
- [3] 论文代码和工件. https://github.com/BU-DiSC/Benchmark-Analyze-Optimize-Partial-Compaction-in-RocksDB-Codebase